{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olive MG\n",
    "\n",
    "# EDA\n",
    "\n",
    "### Lesson 1\n",
    "Analyzing One Variable by Chris Saden\n",
    "Data is becoming  Ubiquitous and cheap - whats complimentary to this - analysis, Facebook processes more than 500 terabyte of data a day (2012\n",
    "\n",
    "EDA - is the examination of data and relationships, through numerial and graphical methods \n",
    "its the begining part of a large process before more formal and rigorous analyses\n",
    "It can lead into insights or more questions or feed into the larger part of predictive modeling\n",
    "Also a good time to check the dataset.  time to check assumptions and understand the data, check for anomalies etc\n",
    "\n",
    "How EDA can fit into the predictive modeling phase\n",
    "eg the Netflix prize competition - Netflix wanted to improve their movie recommendation by at least 10%\n",
    "https://en.wikipedia.org/wiki/Netflix_Prize\n",
    "https://flowingdata.com/2007/12/11/netflix-prize-dataset-visualization/\n",
    "EDA improves your ability to reashon using data, shapens your communication skills and improve your skillset\n",
    "\n",
    "EDA is an approach to understanding data using visualisations and statistical tools. EDA is your initial interaction with data. eg mtcars\n",
    "we use Eda to assess and validate assumptions on which future inferences may be based. \n",
    "-Ie which variables are normally distributed\n",
    "- which variable is biased toward x or y?\n",
    "- you need to understd eda before hypothesis formation\n",
    "- we are developing intuidtion of out dataset ultimately\n",
    "- find which variables have high predictive power and then find the right statistical tools to build predictive models\n",
    "\n",
    "eg  The Growth of Televisions\n",
    "http://flowingdata.com/2009/09/23/tv-size-over-the-past-8-years/\n",
    "he uses histograms , includes the median line in each graph - awesome visualizn!!!\n",
    "\n",
    "He describes the source of data and the caveat about this data - very impt in understanding and making concns about data.  Be curious & skeptical, play about, let the data speak to u as u build intuition about it,  ask qns,dig deepter through visualizns and summaries. He conducted a time series analysis.\n",
    "\n",
    "Eg Aude Explores Coordinated Migration and they add  interactions as well\n",
    "\n",
    "Exploratory DA - let the data surprise you!\n",
    "What do you do first?\n",
    "*Get summaries & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2\n",
    "\n",
    "The Power of R\n",
    "http://blog.revolutionanalytics.com/2013/08/foodborne-chicago.html\n",
    "Foodborne Chicago finds dodgy restaurants with tweets, and R (textcat)\n",
    "\n",
    "R\n",
    "Leading programing language\n",
    "can be builtup analysis line by line and save our code and go back to it later\n",
    "You can easily share your work with others\n",
    "with over 2000 contributed packages to increase its functionality\n",
    "has a large commy of users\n",
    "\n",
    "install.packages('ggplot2', dependencies = TRUE)\n",
    "install.packages('RColorBrewer', dependencies = TRUE)\n",
    "** use library() to load packages once they're installed!\n",
    "library(ggplot2)\n",
    "library(RColorBrewer)\n",
    "Beautiful graph - in Rstudio file\n",
    "\n",
    "Download and install the R programming language FIRST at http://cran.rstudio.com.\n",
    "After you install R, you can download and install RStudio from http://www.rstudio.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-d20aaa0971b5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-d20aaa0971b5>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Make sure your work is reproducible(mantra in R)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Make sure your work is reproducible(mantra in R)\n",
    "\n",
    "Type 'q()' to quit R.\n",
    "\n",
    "setwd(\"C:/work/EDA\")\n",
    "\n",
    "#install.packages('package-name')\n",
    "to see installed packages type\n",
    "\n",
    "installed.packages()\n",
    "\n",
    "#To use a package once installed, load it with\n",
    "\n",
    "library('packagename')\n",
    "remove.packages()  #if you no longer need it\n",
    "help(functionname)\n",
    "args(functionname)\n",
    "help.search('search term in R documn')\n",
    "data() # see preloaded datasets\n",
    "mtcars <- read.csv('cather.txt')\n",
    " # function assumes 1st row is for names of header. if not\n",
    "true add header=FALSE to the command\n",
    "\n",
    "mtcars <- read.csv('cather.txt', header = FALSE)\n",
    "\n",
    "If data uses another character to separate filed, not a comma\n",
    "use the more general read function\n",
    "\n",
    "mydata <- read.table('file.txt, sep='\\t', header=TRUE)\n",
    "\n",
    "For excel - https://readxl.tidyverse.org/\n",
    "\n",
    "others - http://readxl.tidyverse.org/\n",
    "\n",
    "Remote data\n",
    "pew_data <- read.csv('http://bit.ly/11I3iuU')\n",
    "\n",
    "Difficult to remember which package uses what kind of data\n",
    "import - use the riopackage. this has 3 functions: import, export \n",
    "convert based on the file name\n",
    "mydata %1t- import(\"myfile,json\")\n",
    "\n",
    "Financial analysis package Quantmod makes it easy to not only\n",
    "pull in and analyze stock prices but graph them as well - just \n",
    "for lines\n",
    "\n",
    "install.packages('quantmod')\n",
    "library('quantmod')\n",
    "getSymbols('AAPL')\n",
    "barchart(AAPL)\n",
    "\n",
    "# the last couple of weeks data\n",
    "\n",
    "barChart(AAPL, subset='last 14 days')\n",
    "\n",
    "chartSeries(AAPL, subset = 'last 14 days')\n",
    "\n",
    "Or grab a particular date range\n",
    "\n",
    "barChart (AAPL['2013-04-01::2013-04-12'])\n",
    "\n",
    "Quantmod is a very powerful financial analysis package - see link\n",
    "http://www.quantmod.com/examples/intro/\n",
    "\n",
    "other packages\n",
    "twitteR\n",
    "Quandl and rdatamarket to access millions of datasets at Tuandl and \n",
    "data market\n",
    "and several for google analytics eg rga, RGoogleAnalytics and\n",
    "ganalytics\n",
    "\n",
    "\n",
    "Rwmoving unneeded data\n",
    "rm(x)\n",
    "\n",
    "To save entire workspace\n",
    "save.image()\n",
    "\n",
    "or save(variablename, file= \"filename.rda\")\n",
    "\n",
    "load(\"filename\")\n",
    "                     \n",
    "\n",
    "for speedier saving 6 loading check out feather and fst packages\n",
    "https://github.com/wesm/feather\n",
    "http://www.fstpackage.org/\n",
    "\n",
    "to continue - R on computer world\n",
    "https://www.computerworld.com/article/2598083/application-development/app-development-beginner-s-guide-to-r-easy-ways-to-do-basic-data-analysis.html?page=6\n",
    "\n",
    "\n",
    "install.packages('ggplot2', dependencies = TRUE)\n",
    "install.packages('RColorBrewer', dependencies = TRUE)\n",
    "library(ggplot2)\n",
    "library(RColorBrewer)\n",
    "\n",
    "data(\"diamonds\")\n",
    "\n",
    "# create scatterplot of price vs carat color coded by diamond cuts\n",
    "qplot(data= diamonds, x = carat, y = price, color = cut) +\n",
    "  scale_color_brewer(palette = 'Accent')\n",
    "ggsave('scatterplot1.png')\n",
    "\n",
    "http://www.computerworld.com/article/2884322/learn-r-programming-basics-with-our-pdf.html#tk.ctw-infsb\n",
    "\n",
    "22 Free tools for data visulisation and analysis\n",
    "https://www.trifacta.com/products/wrangler/\n",
    "\n",
    "http://www.computerworld.com/s/article/9214755/Chart_and_image_gallery_30_free_tools_for_data_visualization_and_analysis\n",
    "\n",
    "before u analyze and visualise - clean it first with data wrangler(Trifacta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clive MG\n",
    "#Demystifying R\n",
    "\n",
    "# The goal of this file is to introduce you to the\n",
    "# R programming language. Let's start with by unraveling a\n",
    "# little mystery!\n",
    "\n",
    "# 1. Run the code below to create the vector 'udacious'.\n",
    "# You need to highlight all of the lines of the code and then\n",
    "# run it. You should see \"udacious\" appear in the workspace.\n",
    "\n",
    "udacious <- c(\"Chris Saden\", \"Lauren Castellano\",\n",
    "              \"Sarah Spikes\",\"Dean Eckles\",\n",
    "              \"Andy Brown\", \"Moira Burke\",\n",
    "              \"Kunal Chawla\")\n",
    "\n",
    "# You should see something like \"chr[1:7]\" in the 'Environment'\n",
    "# or 'Workspace' tab. This is because you created a 'vector' with\n",
    "# 7 names that have a 'type' of character. The arrow-like\n",
    "# '<-' symbol is the assignment operator in R, similar to the\n",
    "# equal sign '=' in other programming languages. The c() is a\n",
    "# generic function that combines arguments, in this case the\n",
    "# names of people, to form a vector.\n",
    "\n",
    "# A 'vector' is one of the data types in R. Vectors must contain\n",
    "# the same type of data, that is the entries must all be of the\n",
    "# same type: character (most programmers call these strings),\n",
    "# logical (TRUE or FALSE), or numeric.\n",
    "\n",
    "# Print out the vector udacious by running this next line of code.\n",
    "\n",
    "udacious\n",
    "#[1] \"Chris Saden\"       \"Lauren Castellano\" \"Sarah Spikes\"     \n",
    "#[4] \"Dean Eckles\"       \"Andy Brown\"        \"Moira Burke\"      \n",
    "#[7] \"Kunal Chawla\" \n",
    "\n",
    "# Notice how there are numbers next to the output.\n",
    "# Each number corresponds to the index of the entry in the vector.\n",
    "# Chris Saden is the first entry so [1]\n",
    "# Dean Eckles is the fourth entry so [4]\n",
    "# Kunal Chawla is the seventh entry so [7]\n",
    "\n",
    "# Depending on the size of you window you may see different numbers\n",
    "# in the output.\n",
    "\n",
    "# ANOTHER HELPFUL TIP: You can add values to a vector.\n",
    "# Run each line of code one at a time below to see what is happening.\n",
    "\n",
    "numbers <- c(1:10)\n",
    "\n",
    "numbers\n",
    "\n",
    "numbers <- c(numbers, 11:20)\n",
    "\n",
    "numbers\n",
    "\n",
    "setwd(\"C:/work/EDA/lesson3/eda-course-materials(1)/lesson2\")\n",
    "\n",
    "# 2. Replace YOUR_NAME with your actual name in the vector\n",
    "# 'udacious' and run the code. Be sure to use quotes around it.\n",
    "\n",
    "udacious <- c(\"Chris Saden\", \"Lauren Castellano\",\n",
    "              \"Sarah Spikes\",\"Dean Eckles\",\n",
    "              \"Andy Brown\", \"Moira Burke\",\n",
    "              \"Kunal Chawla\", \"Olive MG\")\n",
    "\n",
    "# Notice how R updates 'udacious' in the workspace.\n",
    "# It should now say something like 'chr[1:8]'.\n",
    "\n",
    "# 3. Run the following two lines of code. You can highlight both lines\n",
    "# of code and run them.\n",
    "\n",
    "mystery = nchar(udacious)\n",
    "mystery\n",
    "\n",
    "# You just created a new vector called mystery. What do you\n",
    "# think is in this vector? (scroll down for the answer)\n",
    "\n",
    "\n",
    "# Mystery is a vector that contains the number of characters\n",
    "# for each of the names in udacious, including your name.\n",
    "\n",
    "# 4. Run this next line of code.\n",
    "\n",
    "mystery == 11\n",
    "\n",
    "# Here we get a logical (or boolean) vector that tells us\n",
    "# which locations or indices in the vector contain a name\n",
    "# that has exactly 11 characters.\n",
    "\n",
    "# 5. Let's use this boolean vector, mystery, to subset our\n",
    "# udacious vector. What do you think the result will be when\n",
    "# running the line of code below?\n",
    "\n",
    "# Think about the output before you run this next line of code.\n",
    "# Notice how there are brackets in the code. Brackets are often\n",
    "# used in R for subsetting.\n",
    "\n",
    "udacious[mystery == 11]\n",
    "\n",
    "# Scroll down for the answer\n",
    "\n",
    "# It's your Udacious Instructors for the course!\n",
    "# (and you may be in the output if you're lucky enough\n",
    "# to have 11 characters in YOUR_NAME) Either way, we\n",
    "# think you're pretty udacious for taking this course.\n",
    "\n",
    "# 6. Alright, all mystery aside...let's dive into some data!\n",
    "# The R installation has a few datasets already built into it\n",
    "# that you can play with. Right now, you'll load one of these,\n",
    "# which is named mtcars.\n",
    "\n",
    "# Run this next command to load the mtcars data.\n",
    "\n",
    "data(mtcars)\n",
    "\n",
    "\n",
    "# You should see mtcars appear in the 'Environment' tab with\n",
    "# <Promise> listed next to it. \n",
    "\n",
    "# The object (mtcars) appears as a 'Promise' object in the\n",
    "# workspace until we run some code that uses the object.\n",
    "\n",
    "# R has stored the mtcars data into a spreadsheet-like object\n",
    "# called a data frame. Run the next command to see what variables\n",
    "# are in the data set and to fully load the data set as an\n",
    "# object in R. You should see <Promise> disappear when you\n",
    "# run the next line of code.\n",
    "\n",
    "# Visit http://cran.r-project.org/doc/manuals/r-release/R-lang.html#Promise-objects\n",
    "# if you want the expert insight on Promise objects. You won't\n",
    "# need to the info on Promise objects to be successful in this course.\n",
    "\n",
    "names(mtcars)\n",
    "\n",
    "# names(mtcars) should output all the variable\n",
    "# names in the data set. You might notice that the car names\n",
    "# are not a variable in the data set. The car names have been saved\n",
    "# as row names. More on this later.\n",
    "\n",
    "# You should also see how many observations (obs.) are in the\n",
    "# the data frame and the number of variables on each observation.\n",
    "\n",
    "# 7. To get more information on the data set and the variables\n",
    "# run the this next line of code.\n",
    "\n",
    "?mtcars\n",
    "\n",
    "# You can type a '?' before any command or a data set to learn\n",
    "# more about it. The details and documentation will appear in\n",
    "# the 'Help' tab.\n",
    "\n",
    "\n",
    "# 8. To print out the data, run this next line as code.\n",
    "\n",
    "mtcars\n",
    "\n",
    "# Scroll up and down in the console to check out the data.\n",
    "# This is the entire data frame printed out.\n",
    "\n",
    "# 9. Run these next two functions, one at a time,\n",
    "# and see if you can figure out what they do.\n",
    "\n",
    "str(mtcars)\n",
    "\n",
    "dim(mtcars)\n",
    "\n",
    "# Scroll down for the answer.\n",
    "\n",
    "\n",
    "# The first command, str(mtcars), gives us the structure of the\n",
    "# data frame. It lists the variable names, the type of each variable\n",
    "# (all of these variables are numerics) and some values for each\n",
    "# variable.\n",
    "\n",
    "\n",
    "# The second command, dim(mtcars), should output '[1] 32 11'\n",
    "# to the console. The [1] indicates that 32 is the first value\n",
    "# in the output.\n",
    "\n",
    "#****** R uses 1 to start indexing (AND NOT ZERO BASED INDEXING as is true\n",
    "# of many other programming languages.)\n",
    "\n",
    "# 10. Read the documentation for row.names if you're want to know more.\n",
    "?row.names\n",
    "\n",
    "# Run this code to see the current row names in the data frame.\n",
    "row.names(mtcars)\n",
    "\n",
    "# Run this code to change the row names of the cars to numbers.\n",
    "row.names(mtcars) <- c(1:32)\n",
    "\n",
    "# Now print out the data frame by running the code below.\n",
    "mtcars\n",
    "\n",
    "# It's tedious to relabel our data frame with the right car names\n",
    "# so let's reload the data set and print out the first ten rows.\n",
    "\n",
    "data(mtcars)\n",
    "head(mtcars, 10)\n",
    "\n",
    "# The head() function prints out the first six rows of a data frame\n",
    "# by default. Run the code below to see.\n",
    "head(mtcars)\n",
    "\n",
    "# I think you'll know what this does.\n",
    "tail(mtcars, 3)\n",
    "\n",
    "\n",
    "# 11. We've run nine commands so far:\n",
    "#      c, nchar, data, str, dim, names, row.names, head, and tail.\n",
    "\n",
    "# All of these commands took some inputs or arguments.\n",
    "# To determine if a command takes more arguments or to learn\n",
    "# about any default settings, you can look up the documentation\n",
    "# using '?' before the command, much like you did to learn about\n",
    "# the mtcars data set and the row.names\n",
    "\n",
    "\n",
    "\n",
    "# 12. Let's examine our car data more closely. We can access an\n",
    "# an individual variable (or column) from the data frame using\n",
    "# the '$' sign. Run the code below to print out the variable\n",
    "# miles per gallon. This is the mpg column in the data frame.\n",
    "\n",
    "mtcars$mpg\n",
    "\n",
    "# Print out any two other variables to the console.\n",
    "\n",
    "# This is a vector containing the mpg (miles per gallon) of\n",
    "# the 32 cars. Run this next line of code to get the average mpg for\n",
    "# for all the cars. What is it?\n",
    "\n",
    "# Enter this number for the quiz on the Udacity website.\n",
    "# https://www.udacity.com/course/viewer#!/c-ud651/l-729069797/e-804129314/m-830829287\n",
    "\n",
    "mean(mtcars$mpg)\n",
    "\n",
    "mean(mtcars$cyl)\n",
    "\n",
    "mean(mtcars$gears)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Olive MG  07/10/2018\n",
    "\n",
    "## Demystifying R Part 2\n",
    "========================================================\n",
    "\n",
    "```{r}\n",
    "str(mtcars)\n",
    "```\n",
    "\n",
    "The str() and summary() functions are helpful commands when working with a new data set.\n",
    "The str() function gives us the variable names and their types.\n",
    "The summary() function gives us an idea of the values a variable can take on.\n",
    "\n",
    "2. In 2013, the average mpg (miles per gallon) for a car was 23 mpg.\n",
    "The car models in the mtcars data set come from the year 1973-1974.\n",
    "Subset the data so that you create a new data frame that contains\n",
    "cars that get 23 or more mpg (miles per gallon). Save it to a new data\n",
    "frame called efficient.\n",
    "\n",
    "```{r}\n",
    "efficient <- subset(mtcars, mpg >= 23)\n",
    "```\n",
    "\n",
    "3. How many cars get more than 23 mpg? Use one of the commands you\n",
    "learned in the demystifying.R to answer this question.\n",
    "\n",
    "```{r}\n",
    "Num_cars_above_23mpg <- subset(mtcars, mpg > 23)\n",
    "```\n",
    "\n",
    "4. We can also use logical operators to find out which car(s) get greater\n",
    "than 30 miles per gallon (mpg) and have more than 100 raw horsepower.\n",
    "\n",
    "```{r}\n",
    "subset(mtcars, mpg > 30 & hp > 100)\n",
    "```\n",
    "\n",
    "There's only one car that gets more than 30 mpg and 100 hp.\n",
    "\n",
    "5. What do you think this code does? Scroll down for the answer.\n",
    "\n",
    "```{r}\n",
    "subset(mtcars, mpg < 14 | disp > 390)\n",
    "```\n",
    "\n",
    "Note: You may be familiar with the || operator in Java. R uses one single & for the logical\n",
    "operator AND. It also uses one | for the logical operator OR.\n",
    "\n",
    "\n",
    "The command above creates a data frame of cars that have mpg less than 14\n",
    "OR a displacement of more than 390. Only one of the conditions for a car\n",
    "needs to be satisfied so that the car makes it into the subset. Any of the\n",
    "cars that fit the criteria are printed to the console.\n",
    "\n",
    "Now you try some.\n",
    "\n",
    "6. Print the cars that have a 1/4 mile time (qsec) less than or equal to\n",
    "16.90 seconds to the console.\n",
    "\n",
    "```{r}\n",
    "subset(mtcars, qsec <= 16.90)\n",
    "\n",
    "```\n",
    "\n",
    "7. Save the subset of cars that weigh under 2000 pounds (weight is measured in lb/1000)\n",
    "to a variable called lightCars. Print the numbers of cars and the subset to the console. ????? - not right???\n",
    "\n",
    "```{r}\n",
    "lightcars <- subset(mtcars, wt < 2.0)\n",
    "\n",
    "#nrows(mtcars)????\n",
    "lightcars\n",
    "\n",
    "```\n",
    "\n",
    "8. You can also create new variables in a data frame. Let's say you wanted\n",
    "to have the year of each car's model. We can create the variable\n",
    "mtcars$year. Here we'll assume that all of the models were from 1974.\n",
    "Run the code below.\n",
    "\n",
    "```{r}\n",
    "mtcars$year <- 1974\n",
    "\n",
    "#mtcars$color <- 'black'\n",
    "```\n",
    "\n",
    "Notice how the number of variables changed in the work space. You can\n",
    "also see the result by double clicking on mtcars in the workspace and\n",
    "examining the data in a table.\n",
    "\n",
    "To drop a variable, subset the data frame and select the variable you\n",
    "want to drop with a negative sign in front of it.\n",
    "\n",
    "```{r}\n",
    "mtcars <- subset(mtcars, select = - year)\n",
    "\n",
    "```\n",
    "\n",
    "Notice, we are back to 11 variables in the data frame.\n",
    "\n",
    "9. What do you think this code does? Run it to find out.\n",
    "\n",
    "```{r}\n",
    "mtcars$year <- c(1973, 1974)\n",
    "```\n",
    "\n",
    "Open the table of values to see what values year takes on.\n",
    "\n",
    "Drop the year variable from the data set.\n",
    "```{r}\n",
    "mtcars$year <- subset(mtcars, select = - year)\n",
    "```\n",
    "\n",
    "\n",
    "10. Now you are going to get a preview of ifelse(). For those new\n",
    "to programming this example may be confusing. See if you can understand\n",
    "the code by running the commands one line at a time. Read the output and\n",
    "make sense of what the code is doing at each step.\n",
    "\n",
    "If you are having trouble don't worry, we will review the ifelse statement\n",
    "at the end of Lesson 3. You won't be quizzed on it, and it's not essential\n",
    "to keep going in this course. We just want you to try to get familiar with\n",
    "more code.\n",
    "\n",
    "\n",
    "```{r}\n",
    "mtcars$wt\n",
    "cond <- mtcars$wt < 3\n",
    "cond\n",
    "weight_class <- ifelse(cond, 'light', 'average')\n",
    "weight_class\n",
    "mtcars$weight_class\n",
    "cond <- mtcars$wt > 3.5\n",
    "mtcars$weight_class <- ifelse(cond, 'heavy', 'average') \n",
    "mtcars$weight_class\n",
    "```\n",
    "\n",
    "You have some variables in your workspace or environment like 'cond' and\n",
    "efficient. You want to be careful that you don't bring in too much data\n",
    "into R at once since R will hold all the data in working memory. We have\n",
    "nothing to worry about here, but let's delete those variables from the\n",
    "work space.\n",
    "\n",
    "```{r}\n",
    "rm(cond)\n",
    "rm(efficient)\n",
    "```\n",
    "\n",
    "Save this file if you haven't done so yet.\n",
    "\n",
    "\n",
    "You'll have the opportunity to create one Rmd file for the final project in\n",
    "this class and submit the Rmd file and knitted output (or HTML file). You'll\n",
    "need the knitr package to do that so let's install that now. **Uncomment** the\n",
    "following two lines of code and run them.\n",
    "\n",
    "```{r}\n",
    "# install.packages('knitr', dependencies = T)\n",
    "# library(knitr)\n",
    "```\n",
    "\n",
    "Once you've installed knitr, **comment** out the two lines of code above.\n",
    "When you click the **Knit HTML** button a web page will be generated that\n",
    "includes both content (text and text formatting from Markdown) as well as\n",
    "the output of any embedded R code chunks within the document.\n",
    "\n",
    "\n",
    "You've reached the end of the file so now it's time to write some code to\n",
    "answer a question to continue on in Lesson 2.\n",
    "\n",
    "Which car(s) have an mpg (miles per gallon) greater than or equal to 30\n",
    "OR hp (horsepower) less than 60? Create an R chunk of code to answer the question.\n",
    "\n",
    "\n",
    "\n",
    "Once you have the answer, go the [Udacity website](https://www.udacity.com/course/viewer#!/c-ud651/l-729069797/e-804129319/m-811719066) to continue with Lesson 2.\n",
    "\n",
    "Note: You use brackets around text followed by two parentheses to create a link.\n",
    "There must be no spaces between the brackets and the parentheses. Paste or type\n",
    "the link into the parentheses. This also works on the discussions!\n",
    "\n",
    "And if you want to see all of your HARD WORK from this file, click\n",
    "the **KNIT HTML** button now. (You may or may not need to restart R).\n",
    "\n",
    "# CONGRATULATIONS\n",
    "#### You'll be exploring data soon with your new knowledge of R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-5ad455605718>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-5ad455605718>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Olive MG  07/10/2018\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Olive MG  07/10/2018\n",
    "\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson2\")\n",
    "reddit.csv')\n",
    "\n",
    "reddit <- read.csv('reddit.csv')\n",
    "\n",
    "rm(statesinfo)\n",
    "\n",
    "## Factor Variables\n",
    "table(reddit$employment.status)\n",
    "http://stats.idre.ucla.edu/r/modules/factor-variables/\n",
    "\n",
    "summary(reddit)\n",
    "\n",
    "Data typtes in R - https://www.statmethods.net/input/datatypes.html\n",
    "\n",
    "Setting Levels of Ordered Factors\n",
    "# Order the factor levels in the age.range variable in order to create\n",
    "# a graph with a natural order. Look up the documentation for\n",
    "# the factor function or read through the example in the Instructor Notes.\n",
    "\n",
    "# Once you're ready, try to write the code to order the levels of\n",
    "# the age.range variable.\n",
    "\n",
    "# Be sure you modify the variable in the data frame. That is modify reddit$age.range.\n",
    "# Don't create a new variable.\n",
    "\n",
    "# The levels of age.range should take on these values...\n",
    "\n",
    "#    \"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65 or Above\"\n",
    "\n",
    "# This exercise is ungraded. You can check your own work by using the Test Run\n",
    "# button. Your plot will appear there.\n",
    "\n",
    "# ENTER YOUR CODE BELOW THE LINE.\n",
    "# ================================================================================\n",
    "i\n",
    "setting levels of ordered factors \n",
    "# first rearrange the levels as in y, then set the levels with the orderd function,\n",
    "# then rerun the plot function again\n",
    "y <- c(\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65 or Above\")\n",
    "\n",
    "reddit$age.range <- ordered(reddit$age.range, levels = y)\n",
    "\n",
    "qplot(data = reddit, x = age.range)\n",
    "\n",
    "#Alternative solution\n",
    "reddit$age.range <- factor(reddit$age.range, levels = y, ordered = T)\n",
    "\n",
    "------\n",
    "ii  Ordering Income.range \n",
    "\n",
    "# i) rearrange levels\n",
    "w = c(\"Under $20,000\", \"$20,000 - $29,999\", \"$30,000 - $39,999\",\n",
    "      \"$40,000 - $49,999\", \"$50,000 - $69,999\", \"$70,000 - $99,999\", \n",
    "      \"$100,000 - $149,999\", \"$150,000 or more\")\n",
    "\n",
    "# set the levels\n",
    "reddit$income.range <- ordered(reddit$income.range,levels = w)\n",
    "\n",
    "# Plot \n",
    "qplot(data = reddit, x=income.range)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3\n",
    "example - https://rpubs.com/anthonycerna/data_analysis_with_r_lesson_one\n",
    "https://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/\n",
    "\n",
    "\n",
    "getwd()\n",
    "\n",
    "list.files()\n",
    "\n",
    "pf <- read.csv(\"pseudo_facebook.tsv\", sep = '\\t')\n",
    "\n",
    "names(pf)\n",
    "\n",
    "How to read histograms - https://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b5813229989b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b5813229989b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Lesson 3\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Lesson 3\n",
    "========================================================\n",
    "https://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/\n",
    "***\n",
    "\n",
    "### What to Do First?\n",
    "Notes:  set your working dir.\n",
    "\n",
    "***\n",
    "\n",
    "### Pseudo-Facebook User Data\n",
    "read the data in, note '\\t' tab separated file\n",
    "Notes:\n",
    "\n",
    "```{r Pseudo-Facebook User Data}\n",
    "pf <- read.csv('pseudo_facebook.tsv', sep='\\t')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Histogram of Users' Birthdays\n",
    "Notes:\n",
    "You can use scale_x_continuous() instead to get the break points, or use ggplot() syntax *.**** did not work?? for me   \n",
    "\n",
    "```{r Histogram of Users\\' Birthdays}\n",
    "#install.packages('ggplot2')\n",
    "library(ggplot2)\n",
    "\n",
    "\n",
    "qplot(x = dob_day, data = pf) +\n",
    "scale_x_continuous()\n",
    "\n",
    "ggplot(aes(x = dob_day), data = pf) +\n",
    "  geom_histogram(binwidth = 1) +\n",
    "  scale_x_continuous(breaks = 1:31)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### What are some things that you notice about this histogram?\n",
    "Response: The spike on day 1\n",
    "\n",
    "***\n",
    "\n",
    "### Moira's Investigation\n",
    "Notes:\n",
    "Audience higher than the percieved number\n",
    "\n",
    "***\n",
    "\n",
    "### Estimating Your Audience Size\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "#### Think about a time when you posted a specific message or shared a photo on Facebook. What was it?\n",
    "Response:0\n",
    "\n",
    "#### How many of your friends do you think saw that post?\n",
    "Response:0\n",
    "\n",
    "#### Think about what percent of your friends on Facebook see any posts or comments that you make in a month. What percent do you think that is?\n",
    "Response:\n",
    "\n",
    "***\n",
    "\n",
    "### Perceived Audience Size\n",
    "Notes:\n",
    "\n",
    "***\n",
    "### Faceting\n",
    "Notes:  there 2 types  In this case the above histogram is divided into  12 histograms based on the months using facet-wrap (for one variable)\n",
    "facet_wrap(~ variable)\n",
    "facet_grid(vertical~horizontal) # 2 or more variables\n",
    "\n",
    "for more than one variable, use facet-grid.\n",
    "```{r Faceting}\n",
    "qplot(x = dob_day, data = pf) +\n",
    "  scale_x_continuous(breaks = 1:31) + \n",
    "  facet_wrap(~dob_month, ncol = 3)\n",
    "  \n",
    "```\n",
    "\n",
    "ggplot(data = pf, aes(x = dob_day)) +\n",
    "  geom_histogram(binwidth = 1) +\n",
    "  scale_x_continuous(breaks = 1:31) +\n",
    "  facet_wrap(~dob_month)\n",
    "\n",
    "\n",
    "#### Let’s take another look at our plot. What stands out to you here?\n",
    "Response:\n",
    "\n",
    "***\n",
    "\n",
    "### Be Skeptical - Outliers and Anomalies - there types due to different causes eg\n",
    "an extreme case like someone tweets 1000 times/day\n",
    "or they may represent bad data or the limitations of the data\n",
    "or extreme cases may be replaced with normal values like in census data where extreme salaries are brought down with normal values\n",
    "\n",
    "Types/ category impt inorder to know how to exclude it\n",
    "*bad data about a non-extreme case\n",
    "*bad data about an extreme case\n",
    "*good data about an extreme case\n",
    "\n",
    "***\n",
    "\n",
    "### Moira's Outlier\n",
    "Notes: terrible plot coz one person guessed(percieved a number in millions !!!  So she had to adjust the axes first and foremost so that she could see the bulk of the data)\n",
    "#### Which case do you think applies to Moira’s outlier?\n",
    "Response:\n",
    "\n",
    "***\n",
    "\n",
    "### Friend Count\n",
    "Notes:\n",
    "\n",
    "#### What code would you enter to create a histogram of friend counts?\n",
    "\n",
    "```{r Friend Count}\n",
    "qplot(x = friend_count, data = pf)\n",
    "```\n",
    "\n",
    "#### How is this plot similar to Moira's first plot?\n",
    "Response:\n",
    "Its also long tail histogram with much of the data squished on the left\n",
    "\n",
    "***\n",
    "\n",
    "### Limiting the Axes\n",
    "Notes: Therefore we have to limit the axes to see the data clearly.  Say we want to see data within the first 1000 users. Use xlim as a vector\n",
    "\n",
    "```{r Limiting the Axes}\n",
    "qplot(x = friend_count, data = pf, xlim = c(0,1000))\n",
    "\n",
    "#alternatively\n",
    "\n",
    "qplot(x = friend_count, data = pf) +\n",
    "  scale_x_continuous(limits = c(0, 1000))\n",
    "\n",
    "```\n",
    "\n",
    "### Exploring with Bin Width\n",
    "Notes:\n",
    "qplot(x = friend_count, data = pf, binwidth = 25) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000,50))\n",
    "***\n",
    "\n",
    "### Adjusting the Bin Width\n",
    "Notes:\n",
    "setting it at 1 allows you to see the individual perceptions that comeup as spikes\n",
    "\n",
    "### Faceting Friend Count - say we want to know which gender has the highest count\n",
    "# What code would you add to create a facet the histogram by gender?\n",
    "# Add it to the code below\n",
    "\n",
    "```{r Faceting Friend Count}\n",
    "qplot(x = friend_count, data = pf, binwidth = 25) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +\n",
    "  facet_wrap(~gender)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Omitting NA Values\n",
    "https://www.statmethods.net/input/missingdata.html\n",
    "Notes: we add a condition to omit the NA values only within gender(!is.na(gender))\n",
    "\n",
    "```{r Omitting NA Values}\n",
    "#qplot(x = friend_count, data = subset(pf, !is.na(gender)),binwidth = 10) +\n",
    "#  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +\n",
    "#  facet_wrap(~gender)\n",
    "\n",
    "#alternatively - note: be careful with na.omit() coz it may remove na thats #not relate to the variable you want removed\n",
    "\n",
    "qplot(x = friend_count, data = na.omit(pf), binwidth = 10) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +\n",
    "  facet_wrap(~gender)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Statistics 'by' Gender\n",
    "Notes:\n",
    "\n",
    "```{r Statistics \\'by\\' Gender}\n",
    "table(pf$gender)\n",
    "by(pf$friend_count, pf$gender, summary)\n",
    "```\n",
    "\n",
    "#### Who on average has more friends: men or women?\n",
    "Response: Women\n",
    "\n",
    "#### What's the difference between the median friend count for women and men?\n",
    "Response:\n",
    "22\n",
    "#### Why would the median be a better measure than the mean?\n",
    "Response:\n",
    "more robust\n",
    "***\n",
    "\n",
    "### Tenure\n",
    "Notes:\n",
    "\n",
    "```{r Tenure}\n",
    "qplot(x = tenure, data = pf, binwidth = 30,\n",
    "      color = I('black'), fill = I('#099DD9'))\n",
    "```\n",
    "\n",
    "***The parameter color determines the color outline of objects in a plot.\n",
    "\n",
    "The parameter fill determines the color of the area inside objects in a plot.\n",
    "\n",
    "You might notice how the color black and the hex code color of #099DD9 (a shade of blue) are wrapped inside of I(). \n",
    "    The I() functions stand for 'as is' and tells qplot to use them as colors.\n",
    "\n",
    "#### How would you create a histogram of tenure by year?\n",
    "\n",
    "```{r Tenure Histogram by Year}\n",
    "qplot(x = tenure/365, data = pf, binwidth = 0.25,\n",
    "      color = I('black'), fill = I('#099DD9')) +\n",
    "  scale_x_continuous(breaks = seq(1, 7, 1), limits = c(0, 7))\n",
    "\n",
    "#`change binwidth from 1, to 10, to 0.25\n",
    "\n",
    "```\n",
    "\n",
    "The bulk of the users had less than 2,5 years in fb.  To improve the plot - change the x axis so that it increments by 1 year.\n",
    "    We also limit users to only 7\n",
    "\n",
    "http://docs.ggplot2.org/0.9.2.1/theme.html\n",
    "\n",
    "Equivalent ggplot syntax for plots:\n",
    "\n",
    "ggplot(aes(x = tenure), data = pf) +\n",
    "  geom_histogram(binwidth = 30, color = 'black', fill = '#099DD9')\n",
    "\n",
    "ggplot(aes(x = tenure/365), data = pf) +\n",
    "  geom_histogram(binwidth = .25, color = 'black', fill = '#F79420')\n",
    "\n",
    "\n",
    "*** \n",
    "\n",
    "### Labeling Plots\n",
    "Notes:\n",
    "\n",
    "```{r Labeling Plots}\n",
    "qplot(x = tenure/365, data = pf, binwidth = 0.25,\n",
    "      xlab = 'Number of years using Facebook',\n",
    "      ylab = 'Number of users in sample',\n",
    "      color = I('black'), fill = I('#099DD9')) +\n",
    "  scale_x_continuous(breaks = seq(1, 7, 1), lim = c(0, 7))\n",
    "\n",
    "#or\n",
    "\n",
    "ggplot(aes(x=tenure/365), data=pf) +\n",
    "  geom_histogram(color=I('black'), fill=I('dark green'), binwidth=.25) +\n",
    "  scale_x_continuous(breaks=seq(1,7,1), limits=c(0,7)) +\n",
    "  labs(x=\"Number of years using Facebook\", y='Number of users in sample')\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "names(pf)\n",
    "\n",
    "### User Ages\n",
    "Notes:\n",
    " or ggplot(aes(x = age), data = pf) +\n",
    "  geom_histogram(binwidth = 1, fill = '#5760AB') +\n",
    "  scale_x_continuous(breaks = seq(0, 113, 5))\n",
    "  \n",
    "```{r User Ages}\n",
    "min(pf$age)\n",
    "max(pf$age)\n",
    "\n",
    "qplot(x = age, data = pf, binwidth = 1,\n",
    "      xlab = 'User age',\n",
    "      ylab = 'count',\n",
    "      color = I('black'), fill = I('#099DD9')) +\n",
    "  scale_x_continuous(breaks = seq(0, 113, 5))\n",
    "\n",
    "#users must be at least 13 years of age to set up a Facebook account, #which is why there is no data below 13.\n",
    " \n",
    "\n",
    "```\n",
    "\n",
    "#### What do you notice?\n",
    "Response:\n",
    "Response: there is a bell shape curve with a long right tail. The number of users increases from age 13 and it appears to \n",
    "    peak around the age of 20, then the number of users begins to decrease after the age of 21. There is also those large \n",
    "    spikes (anomalies) after the age of 100. Those are most likely fake user ages that are reported\n",
    "***\n",
    "\n",
    "### The Spread of Memes\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "### Lada's Money Bag Meme\n",
    "Notes:   She is interested in how information flows through networks (i.e. social networks). Memes tend to replicate themselves, especially when they have text that say “repost” or “copy and paste”.\n",
    "\n",
    "In order to analyze the occurrance of moneybag mean, Lada attempted to plot the occurrances of this meme. And she saw various \n",
    "    spikes particularly in the months that were considered to be “lucky” because they had 5 fridays, saturdays, and sundays. \n",
    "    When looking at her plots on a linear scale, using linear counts, it appears that the mean dissapears in the areas where\n",
    "    the spikes are not visible. The meme probably never disspeared and it might have just been floating around facebook in low \n",
    "    numbers.\n",
    "\n",
    "To check this, one can use a log scale and the pattern is much more evident. Using this, we can see counts that are of size 10\n",
    "    while also seeing counts that are of 100,000. Eventhough there is a rapid decay of interest, it actually looks like it \n",
    "    might be parallel. This was done in ggplot using a simple line geome, and grouping by the particular meme variant, and then\n",
    "    rescaling the yaxis to one of the log versions.\n",
    "\n",
    "***\n",
    "qplot(x= friend_count, data = pf)\n",
    "\n",
    "### Transforming Data\n",
    "Notes: Most variables like friend count, likes, comments, wall posts and others are variables called ENGAGEMENT VARIABLES with \n",
    "    very long tails. Some have 10 times or even 100 times the median value. They are in oRDER OF MAGNITUDES, i.e have more \n",
    "    likes, clicks, or comments, than any other users. In statistics, we say that the data is OVER DISPERSED. Often, it helps to \n",
    "    transform these values so we can see standard deviations, or orders of magnitudes, so we are in effect, shortening the tail.\n",
    "```{r}\n",
    "summary(pf$friend_count)\n",
    "\n",
    "```\n",
    "\n",
    "Ex: The histogram of the friend count had very long tails. We can transform the data useing a log, log base 2, or base 10. We \n",
    "    could also use the square root, and doing so helps us to see patterns more clearly without being distracted by the tails. \n",
    "    Alot of common statistical techniques like linear regression, are based on the assumption that variables have normal \n",
    "    distributions. So, by taking the log of this variable, we can transform our data to turn it into a normal distribution or \n",
    "    something that more closely resembles a normal distribution.\n",
    "```{r}\n",
    "summary(pf$friend_count)\n",
    "\n",
    "summary(log10(pf$friend_count))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Trying a log10 transformation, we get something unusual since we get negative infinity for both the minimum and mean. Note, \n",
    "    some of our users have a friend count of zero. So when we take the log10 of 0, that would be undefined. Using calculus,\n",
    "    we would get that the limit would be -Inf. To avoid this, we are going to add 1 to friend count, so that we don’t get an\n",
    "    undefined answer.\n",
    "\n",
    "We can also use the sqrt transformation. The instructor mentions that log10 is an easier tranformation to wrap his head around,\n",
    "    since he is just comparing friend counts on orders of magnitude of 10. Basically, a 10 fold scale, like the pH scale.\n",
    "\n",
    "```{r}\n",
    "summary(log10(pf$friend_count +1))\n",
    "\n",
    "summary(sqrt(pf$friend_count))\n",
    "```\n",
    "\n",
    "Create Multiple Plots in One Image Output \n",
    "http://lightonphiri.org/blog/ggplot2-multiple-plots-in-one-graph-using-gridextra\n",
    "\n",
    "Add Log or Sqrt Scales to an Axis\n",
    "http://docs.ggplot2.org/current/scale_continuous.html\n",
    "\n",
    "Assumptions of Linear Regression\n",
    "http://en.wikipedia.org/wiki/Linear_regression#Assumptions\n",
    "\n",
    "Normal Distribution\n",
    "http://en.wikipedia.org/wiki/Normal_distribution\n",
    "\n",
    "You need to run the following lines of code before trying to create all three histograms on one plot.\n",
    "\n",
    "install.packages('gridExtra')\n",
    "library(gridExtra)\n",
    "\n",
    "\n",
    "\n",
    "Log Transformations of Data\n",
    "http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/\n",
    "\n",
    "\n",
    "```{r}\n",
    "library(gridExtra)\n",
    "\n",
    "p1 <- ggplot(aes(x=friend_count), data=pf) +\n",
    "  geom_histogram()\n",
    "p2 <- ggplot(aes(x=log10(friend_count+1)), data=pf) +\n",
    "  geom_histogram()\n",
    "p3 <- ggplot(aes(x=sqrt(friend_count)), data=pf) +\n",
    "  geom_histogram()\n",
    "grid.arrange(p1,p2,p3, ncol=1)\n",
    "```\n",
    "\n",
    "#Alternative plotting method (different x-axis than the one above): \n",
    "\n",
    "```{r}\n",
    "p1 <- ggplot(aes(x=friend_count), data=pf) + geom_histogram()\n",
    "p2 <- p1 + scale_x_log10()\n",
    "p3 <- p1 + scale_x_sqrt()\n",
    "grid.arrange(p1,p2,p3, ncol=1)\n",
    "```\n",
    "\n",
    "\n",
    "#Note, there is a slight difference here based on the x-axis labeling.\n",
    "\n",
    "***\n",
    "\n",
    "### Add a Scaling Layer\n",
    "Notes:\n",
    "\n",
    "```{r Add a Scaling Layer}\n",
    "logScale <- qplot(x = log10(friend_count), data = pf) \n",
    "\n",
    "countScale <- ggplot(aes(x = friend_count), data = pf) +\n",
    "  geom_histogram() +\n",
    "  scale_x_log10()\n",
    "\n",
    "grid.arrange(logScale, countScale, ncol=2)\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "### Frequency Polygons\n",
    "\n",
    "- good for comparing two or more distributions at once\n",
    "\n",
    "```{r Frequency Polygons} \n",
    "qplot(x = friend_count, data = subset(pf, !is.na(gender)),binwidth = 10,) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +\n",
    "  facet_wrap(~gender)\n",
    "\n",
    "qplot(x = friend_count, data = subset(pf, !is.na(gender)),\n",
    "      binwidth = 10, geom = 'freqpoly', color = gender) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) \n",
    "```\n",
    "\n",
    "Note that the shape of the frequency polygon depends on how our bins are set up - the height of the lines are the same as \n",
    "    the bars in individual histograms, but the lines are easier to make a comparison with since they are on the same axis.\n",
    "\n",
    "Equivalent ggplot syntax:\n",
    "\n",
    "ggplot(aes(x = friend_count, y = ..count../sum(..count..)),\n",
    "       data = subset(pf, !is.na(gender))) +\n",
    "  geom_freqpoly(aes(color = gender), binwidth=10) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +\n",
    "  xlab('Friend Count') +\n",
    "  ylab('Proportion of users with that friend count')\n",
    "  \n",
    "  the question was who has more friends?  To answer that we need to change counts into proportions\n",
    "  \n",
    "```{r}\n",
    "qplot(x = friend_count, y = ..count../sum(..count..),\n",
    "      data = subset(pf, !is.na(gender)),\n",
    "      xlab = 'Friend count',\n",
    "      ylab = 'Proportion of Users with that friend count',\n",
    "      binwidth = 10, geom = 'freqpoly', color = gender) +\n",
    "  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50))\n",
    "\n",
    "```\n",
    "  \n",
    "\n",
    "\n",
    "```{r}\n",
    "qplot(x = friend_count, y = ..count../sum(..count..),\n",
    "      data = subset(pf, !is.na(gender)),\n",
    "      xlab = 'Friend count',\n",
    "      ylab = 'Proportion of Users with that friend count',\n",
    "      binwidth = 10, geom = 'freqpoly', color = gender) +\n",
    "  scale_x_continuous(lim=c(250,1000), breaks=seq(250,1000,50))\n",
    "\n",
    "## use LIMITS or BREAKS to explore more.\n",
    "  #scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50))\n",
    "#scale_x_continuous(lim=c(250,1000), breaks=seq(250,1000,50))\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Likes on the Web\n",
    "Notes:Our above and below frequency plot still does not let us answer our question: who really has more likes, men or women? \n",
    "    Let’s try a numerical summary instead.\n",
    "\n",
    "```{r Likes on the Web}\n",
    "qplot(x = www_likes, data = subset(pf, !is.na(gender)),\n",
    "       geom = 'freqpoly', color = gender) +\n",
    "  scale_x_continuous() + \n",
    "  scale_x_log10()\n",
    "```\n",
    "\n",
    "The first question is asking how many www_likes there are in the entire data set for males.\n",
    "\n",
    "The second question is asking which gender has more www_likes in total.\n",
    "***\n",
    "so we shall find out numerically\n",
    "\n",
    "```{r}\n",
    "by(pf$www_likes, pf$gender, sum)\n",
    "```\n",
    "\n",
    "### Box Plots\n",
    "How to read and use a Boxplot\n",
    "http://flowingdata.com/2008/02/15/how-to-read-and-use-a-box-and-whisker-plot/\n",
    "\n",
    "The interquartile range or IQR includes all of the values between the bottom and top of the boxes in the boxplot.\n",
    "http://en.wikipedia.org/wiki/Interquartile_range\n",
    "\n",
    "Visualization of the IQR with a normal probability distribution function with\n",
    "μ=1\\mu=1μ=1 and σ2=1\\sigma^2=1σ2=1 (pdf).\n",
    "http://en.wikipedia.org/wiki/File:Boxplot_vs_PDF.svg\n",
    "\n",
    "Intro to Descriptive Statistics Exercise: Match Box Plots\n",
    "https://classroom.udacity.com/courses/ud827/lessons/1471748603/concepts/834179180923\n",
    "\n",
    "Notes:\n",
    "\n",
    "```{r Box Plots}\n",
    "qplot(x = gender, y = friend_count,\n",
    "      data = subset(pf, !is.na(gender)),\n",
    "       geom = 'boxplot') \n",
    "\n",
    "```\n",
    "\n",
    "#### Adjust the code to focus on users who have friend counts between 0 and 1000.\n",
    "\n",
    "```{r}\n",
    "qplot(x= gender, y = friend_count, \n",
    "      data = subset(pf, !is.na(gender)), \n",
    "      geom = 'boxplot', ylim= c(0, 1000)) \n",
    "\n",
    "```\n",
    " or \n",
    " \n",
    " #Two methods\n",
    "ggplot(aes(x=gender, y=friend_count), data=subset(pf, !is.na(gender)) ) +\n",
    "      geom_boxplot() +\n",
    "  scale_y_continuous(lim=c(0,1000))\n",
    "  \n",
    "  or \n",
    "  \n",
    "#Cord cartesian method\n",
    "ggplot(aes(x=gender, y=friend_count), data=subset(pf, !is.na(gender)) ) +\n",
    "      geom_boxplot() +\n",
    "  coord_cartesian(ylim=c(0,1000))\n",
    "***\n",
    "\n",
    "### Box Plots, Quartiles, and Friendships\n",
    "Notes:The question is NOT asking who initiated more friendships overall.\n",
    "\n",
    "How to Interpret a Boxplot\n",
    "http://flowingdata.com/2008/02/15/how-to-read-and-use-a-box-and-whisker-plot/\n",
    "\n",
    "The interquartile range or IQR includes all of the values between the bottom and top of the boxes in the boxplot. \n",
    "http://en.wikipedia.org/wiki/Interquartile_range\n",
    "\n",
    "```{r Box Plots, Quartiles, and Friendships}\n",
    "qplot(x= gender, y = friend_count, \n",
    "      data = subset(pf, !is.na(gender)), \n",
    "      geom = 'boxplot') +\n",
    "  coord_cartesian(ylim= c(0, 250))\n",
    "\n",
    "by(pf$friend_count, pf$gender, summary)\n",
    "\n",
    "# NOTE: coord_cartesian allows our box plots to match summary data.\n",
    "\n",
    "```\n",
    "\n",
    "#### On average, who initiated more friendships in our sample: men or women?\n",
    "Response:\n",
    "#### Write about some ways that you can verify your answer.\n",
    "Response:\n",
    "```{r Friend Requests by Gender}\n",
    "qplot(x= gender, y = friendships_initiated, \n",
    "      data = subset(pf, !is.na(gender)), \n",
    "      geom = 'boxplot') + \n",
    "      coord_cartesian(ylim = c(0, 150))\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# Get actual Numbers to check with a numerical summary\n",
    "by(pf$friendships_initiated, pf$gender, summary)\n",
    "```\n",
    "\n",
    "Response:\n",
    "\n",
    "***\n",
    "\n",
    "### Getting Logical\n",
    "Another way of data transformation = converting data w/a lot of zero values to binary (T/F)\n",
    "\n",
    "Notes:There are other ways that we can transform a variable beside using a log or sqrt. You often want to convert variables\n",
    "    that have a lot of 0 values to a new binary variable that has only true and false. This is helpful because we may want to\n",
    "    know if they have used a certain feature at all, instead of the number of times that the user has used that feature. For\n",
    "    example, it may not matter how many times a person checked in using a mobile device, but whether the person has ever used\n",
    "    mobile check in. Using the summary, we see that the median is 4, meaning that we have a lot of zeroes in our dataset.\n",
    "\n",
    "```{r Getting Logical}\n",
    "summary(pf$mobile_likes)\n",
    "\n",
    "summary(pf$mobile_likes > 0)\n",
    "\n",
    "```\n",
    "```{r}\n",
    "# Better still to create a new variable that tracks mobile checkins.\n",
    "pf$mobile_check_in <- NA\n",
    "pf$mobile_check_in <- ifelse(pf$mobile_likes > 0, 1, 0)   # 1 if user has ever used it, 0 if they never have.\n",
    "pf$mobile_check_in <- factor(pf$mobile_check_in)          # Convert it to a factor variable.\n",
    "summary(pf$mobile_check_in)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "#Ratio: What percent of users check in using mobile? Do this programatically.\n",
    "#\n",
    "sum(pf$mobile_check_in ==1) / length(pf$mobile_check_in)\n",
    "```\n",
    "\n",
    "\n",
    "Response: Response: So ~65% of facebook users check in using mobile, which is over half of the users. So it would make a lot of\n",
    "    sense to continue the development of the mobile experience, at least based on this sample of dataset. It is always important not to think about what kind of data you are looking at, but maybe what types of transformations you can make to the variables themselves. Sometimes you want raw counts and other times a binary is prefered.\n",
    "\n",
    "***\n",
    "\n",
    "### Analyzing One Variable\n",
    "Reflection: A lot of this lesson was review for me since I have worked with R a lot in the past. However, there were some key \n",
    "    things that I did learn from this lesson. I learned a lot about the ggplot function, which creates graphs that are far more\n",
    "    aesthetically pleasing than the basic plots found with the default functions from R. I also learned different ways to deal\n",
    "    with long tail distributions and the appropriate way to transform data to better examine trends. I also really enjoyed the\n",
    "    tutorial about how one should play around with bin sizes and overall how to scale a graph to extract as much information as\n",
    "    possible from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lesson 4\n",
    "https://github.com/Halvinn/eda-r-lesson4/commit/d5187b3d67c219ed8f4b46323a577eb9adda164c\n",
    "\n",
    "Problem Set 3: Diamonds\n",
    "\n",
    "This is a file to solve all the excercises of Problem Set 3: Diamonds from Udacity's Course: Data Analysis with R.\n",
    "Frist Excercise:\n",
    "\n",
    "Basic exploration of Diamonds data set.\n",
    "\n",
    "library(ggplot2)\n",
    "data(diamonds)\n",
    "\n",
    "dim(diamonds) # 53940 obs and 10 variables\n",
    "str(diamonds) # 3 Ordered Factors\n",
    "str(diamonds$color)\n",
    "?diamonds # D Represents the best Color\n",
    "\n",
    "Describe the shape and center of the price distribution. Include summary statistics like the mean and median\n",
    "qplot(x = price, data = diamonds)\n",
    "summary(diamonds$price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b23780381410>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b23780381410>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    As a reminder, you can use these commands to load the diamonds data set.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "contd\n",
    "\n",
    "getwd()\n",
    "\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson4\")\n",
    "\n",
    "Diamonds\n",
    "\n",
    "As a reminder, you can use these commands to load the diamonds data set.\n",
    "\n",
    "library(ggplot2) \n",
    "#must load the ggplot package first\n",
    "data(diamonds)\n",
    "#loads the diamonds data set since it comes with the ggplot package\n",
    "\n",
    "You need to run these command each time you launch RStudio to access the diamonds data set. RStudio\n",
    "won't remember which packages and data sets you loaded unless you \n",
    "change your preferences or save your workspace.\n",
    "\n",
    "You should notice that the diamonds data set will be listed as a\n",
    "'Promise' in the workspace. This is a special object in R, and you need \n",
    "to run a command on the data to fully load the data set. Try running a \n",
    "summary:\n",
    "\n",
    "summary(diamonds)\n",
    "\n",
    "str(diamonds)\n",
    "\n",
    "qplot(x = price, data = diamonds)\n",
    "\n",
    "summary(diamonds$price)\n",
    "\n",
    "How many diamonds cost less than $500?\n",
    "nrow(subset(diamonds, price < 500))\n",
    "\n",
    "How many diamonds cost less than $250?\n",
    "nrow(subset(diamonds, price < 250))\n",
    "\n",
    "How many diamonds cost $15000 or more?\n",
    "nrow(subset(diamonds, price >= 15000))\n",
    "\n",
    "# Scales and Multiple Histograms\n",
    "\n",
    "qplot(x = price, data = diamonds, binwidth = 30) +\n",
    "  scale_x_continuous(limits = c(0, 10000), breaks = seq(0, 10000, 1000)) +\n",
    "  facet_wrap(~cut)\n",
    "ggsave('priceHistogram.png')\n",
    "\n",
    "ggplot(data=diamonds) + geom_histogram(binwidth=30, aes(x=diamonds$price)) + \n",
    "  ggtitle(\"Diamond Price Distribution by Cut\") +\n",
    "  xlab(\"Diamond Price U$\") + \n",
    "  ylab(\"Frequency\") + \n",
    "    facet_wrap(~cut)\n",
    "\n",
    "Which cut has the highest priced diamond?\n",
    "premium\n",
    "\n",
    "Which cut has the lowest priced diamond?\n",
    "Premium and ideal\n",
    "\n",
    "Which cut has the lowest median price?\n",
    "ideal\n",
    "\n",
    "by(diamonds$price, diamonds$cut, summary)\n",
    "\n",
    "qplot(x = price, data = diamonds) + facet_wrap(~cut)\n",
    "\n",
    "The 'Fair' and 'Good' diamonds appear to have \n",
    "# different distributions compared to the better\n",
    "# cut diamonds. They seem somewhat uniform\n",
    "# on the left with long tails on the right.\n",
    "\n",
    "# Let's look in to this more.\n",
    "\n",
    "# Look up the documentation for facet_wrap in R Studio.\n",
    "# Then, scroll back up and add a parameter to facet_wrap so that\n",
    "# the y-axis in the histograms is not fixed. You want the y-axis to\n",
    "# be different for each histogram.\n",
    "\n",
    "qplot(x = price, data = diamonds) + facet_wrap(~cut, scales=\"free_y\")\n",
    "\n",
    "#Price per Carat by Cut\n",
    "\n",
    "Create a histogram of price per carat\n",
    "# and facet it by cut. You can make adjustments\n",
    "# to the code from the previous exercise to get\n",
    "# started.\n",
    "\n",
    "qplot(x = price/carat, data = diamonds) + facet_wrap(~cut, scales=\"free_y\")\n",
    "\n",
    "# Adjust the bin width and transform the scale of the x-axis using log10.\n",
    "qplot(x = price/carat, data = diamonds, binwidth = 50) + facet_wrap(~cut, scales=\"free_y\")\n",
    "\n",
    "qplot(x = price/carat, data = diamonds, binwidth = 0.05) + \n",
    "  facet_wrap(~cut, scales=\"free_y\") +\n",
    "  scale_x_log10()\n",
    "ggsave('priceHistogrampricecut.png')\n",
    "\n",
    "\n",
    "ggplot(diamonds) + geom_histogram(aes(x=price/carat), binwidth = 0.05,\n",
    "                                  color = \"black\", fill = \"lightblue\") +\n",
    "  ggtitle(\"Histogram of Price per Carat, facet by Cut.\") + \n",
    "  scale_x_log10() +\n",
    "  facet_wrap(. ~ cut, scale='free_y')\n",
    "  \n",
    "\n",
    "# Submit your final code when you are ready.\n",
    "Did you find normal-ish distributions and a slightly bimodal\n",
    "distribution for Very Good cuts? \n",
    "  \n",
    "Exercise 10: Investigate the price of diamonds \n",
    "# using box plots,\n",
    "# numerical summaries, and one of the following categorical\n",
    "# variables: cut, clarity, or color.\n",
    "\n",
    "qplot(x = cut, y = price, data = diamonds) + \n",
    "  geom_boxplot() +\n",
    "scale_y_continuous(lim = c(0,5000), breaks = seq(0, 6000, 500)) \n",
    "ggsave('boxplotpricecut.png')\n",
    "\n",
    "#fair costs more than good > very good > premium > ideal\n",
    "\n",
    "qplot(x = clarity, y = price, data = diamonds) + \n",
    "  geom_boxplot() +\n",
    "  scale_y_continuous(lim = c(0,5000), breaks = seq(0, 6000, 500)) \n",
    "ggsave('boxplotpriceclarity.png')\n",
    "#The less clear it is the more it costs\n",
    "\n",
    "qplot(x = color, y = price, data = diamonds) + \n",
    "  geom_boxplot() +\n",
    "  scale_y_continuous(lim = c(0,5000), breaks = seq(0, 6000, 500)) \n",
    "ggsave('boxplotpricecolor.png')\n",
    "\n",
    "# price increases from  D to J with some exceptions like G\n",
    "\n",
    "qplot(x = color, y = price, data = diamonds, geom = \"boxplot\") + \n",
    "  coord_cartesian(ylim = c(0,8000))\n",
    "\n",
    "str(diamonds)\n",
    "\n",
    "#Excercise 11: Interquartile range - IQR\n",
    "\n",
    "a) What is the price range for the middle 50% of diamonds with color D?\n",
    "  by(diamonds$price, diamonds$color, summary)\n",
    "b) What is the price range for the middle 50% of diamonds with color J?\n",
    "  by(diamonds$price, diamonds$color, summary)\n",
    "  \n",
    "c) What is the IQR for diamonds with the best color?\n",
    "  by(diamonds$price, diamonds$color, IQR)\n",
    "d) What is the IQR for diamonds with the worst color?\n",
    "  by(diamonds$price, diamonds$color, IQR)\n",
    "\n",
    "Note: For boxplots, we use the \"y\" parameter for the continuous data \n",
    "and the \"x\" parameter for the categorical data.\n",
    "\n",
    "# Ex. 12 Investigate the price per carat of diamonds across\n",
    "# the different colors of diamonds using boxplots.\n",
    "\n",
    "qplot(x = color, y = price/carat, data = diamonds, fill=color) + \n",
    "  geom_boxplot() +\n",
    "  scale_y_continuous(lim = c(0,6000), breaks = seq(0, 6000, 500)) \n",
    "ggsave('boxplotpricepercarat.png')\n",
    "\n",
    "qplot(x = color, y = price/carat, data = diamonds, geom = \"boxplot\") + \n",
    "  coord_cartesian(ylim = c(0,8000) \n",
    "                  \n",
    "# The price increases from D dark to J bright\n",
    "                  \n",
    "# Carat frequency polygon\n",
    "\n",
    "Investigate the weight of the diamonds (carat) using a frequency polygon. Use different bin widths to see how the frequency polygon changes.\n",
    "Use different bin widths to see how the frequency polygon\n",
    "changes.What carat size has a count greater than 2000?\n",
    "\n",
    "??freqpoly\n",
    "\n",
    "ggplot(diamonds) + \n",
    "  geom_freqpoly(aes(x = carat), binwidth = 0.1) +\n",
    "  scale_x_continuous(breaks = seq(0,5,0.2))\n",
    " ggsave('wgtfreqpoly.png')\n",
    "\n",
    "or\n",
    "\n",
    "qplot(data=diamonds, x=carat, xlab='Carat', ylab='Frequency', \n",
    "      binwidth=0.1, geom='freqpoly') + \n",
    "  scale_x_continuous(breaks=seq(0,5,0.2)) + \n",
    "  scale_y_continuous(breaks=seq(0,12000,2000))\n",
    "****************************************************\n",
    "\n",
    "#Data Wrangling with R\n",
    "\n",
    "Data munging or data wrangling can take up much of a data scientist's or \n",
    "data analyst's time. There are two R packages that make these tasks easier in R:\n",
    "  tidyr and dplyr.\n",
    "\n",
    "tidyr -a package that reshapes the layout of your data\n",
    "\n",
    "dplyr - a package that helps you transform tidy, tabular data\n",
    "\n",
    "Review Data Wrangling in R to get a sense of how these packages allow you to manipulate\n",
    "data. You can use these packages to help you in the next programming task and in your\n",
    "future investigations.\n",
    "\n",
    "There are some useful cheat sheets on RStudio's webpage. The Data Import and Data \n",
    "Transformation cheat sheets available at that site will be especially good references \n",
    "for working with tidyr and dplyr, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gapminder Data\n",
    "http://www.gapminder.org/data/\n",
    "https://docs.google.com/spreadsheets/d/1gJLA2BebcFBf7PBpzz5rhjMHK_or8rJf7PhjiE068YA/pub\n",
    "\n",
    "http://rpubs.com/solver/431968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-7e3690719ea2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-7e3690719ea2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Hans Rosling's 200 Countries, 200 Years, 4 Minutes\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Hans Rosling's 200 Countries, 200 Years, 4 Minutes\n",
    "https://www.youtube.com/watch?v=jbkSRLYSojo\n",
    "\n",
    "\n",
    "##About the data:\n",
    "Aid recieved % of gni\n",
    "indicator_aid received % of gni\n",
    "Net official development assistance (ODA) consists of disbursements of loans made on concessional terms (net of repayments of principal) and grants by official agencies of the members of the Development Assistance Committee (DAC), by multilateral institutions, and by non-DAC countries to promote economic development and welfare in countries and territories in the DAC list of ODA recipients. It includes loans with a grant element of at least 25 percent (calculated at a rate of discount of 10 percent).\n",
    "\n",
    "Source organization(s)\tWorld Bank\t\n",
    "Link to source organization\thttp://data.worldbank.org/indicator\t\n",
    "Palau\t\tThis country is not displayed in the graph due to the extremely high percentage of its aid to its GNI\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "getwd()\n",
    "\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson4\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r }\n",
    "#Load necesary Libraries\n",
    "library(readxl, quietly=TRUE)\n",
    "library(reshape2)\n",
    "library(plyr)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(tidyr)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "#Read in the data\n",
    "aid_received <- read_excel('aid_received.xlsx')\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "#Explore data\n",
    "names(aid_received)\n",
    "\n",
    "str(aid_received)\n",
    "\n",
    "dim(aid_received)\n",
    "\n",
    "summary(aid_received)\n",
    "```\n",
    "\n",
    "\n",
    "### Data cleaning issues:\n",
    "1. gather the years and cases to one column each\n",
    "http://garrettgman.github.io/tidying/\n",
    "2. remove missing incomplete cases\n",
    "3. correct Column names\n",
    "\n",
    "\n",
    "```{r}\n",
    "### Reshape data - to gather years to year\n",
    "aid_received2 <- gather(aid_received, \"year\", \"cases\", 2:52)\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "### Remove Missing values\n",
    "aid_received3 <- aid_received2[complete.cases(aid_received2),]\n",
    "\n",
    "names(aid_received3)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "### Correct column names \n",
    "colnames(aid_received3)[3] <- \"donor_aid\"\n",
    "colnames(aid_received3)[1] <- \"country\"\n",
    "\n",
    "# Check the column names\n",
    "colnames(aid_received3)\n",
    "\n",
    "# check the dataset\n",
    "head(aid_received3)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "#  Summary stats of donor aid\n",
    "summary(aid_received3$donor_aid)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "#  Histogram of donor aid\n",
    "qplot(x = donor_aid, data= aid_received3)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "#  investigating Histogram by binwidth and limiting the axes\n",
    "p1 <- qplot(x = donor_aid, data= aid_received3, binwidth = 1, ylab = 'p1 % Gni aid' ) + \n",
    "scale_x_continuous(lim = c(0,80), breaks = seq(0, 80, 3))\n",
    "\n",
    "p2 <- qplot(x = donor_aid, data= aid_received3, binwidth = 3, ylab = 'p2 % Gni aid' ) + \n",
    "scale_x_continuous(lim = c(0,80), breaks = seq(0, 80, 3))\n",
    "\n",
    "p3 <- qplot(x = donor_aid, data= aid_received3, binwidth = 5, ylab = 'p3 % Gni aid') +\n",
    "scale_x_continuous(lim = c(0,80), breaks = seq(0, 80, 2))\n",
    "grid.arrange(p1, p2, p3, ncol=3)\n",
    "ggsave('histobin_changes.png')\n",
    "\n",
    "```\n",
    "\n",
    "### Observations\n",
    "Decreasing the size of the bin increases the clarity of the plots and decreases the values of the y-axes.\n",
    "\n",
    "Since the distribution is longtailed to the right, it calls for transformation so that we can see the data clearly without the tails!\n",
    "\n",
    "\n",
    "```{r}\n",
    "# Transormation using log10 and sqrt\n",
    "# Add a Scaling Layer\n",
    "\n",
    "p1 <- qplot(x = donor_aid, data= aid_received3, binwidth = 1) \n",
    "p2 <- p1 + scale_x_log10()\n",
    "p3 <- p1 + scale_x_sqrt()\n",
    "\n",
    "grid.arrange(p1,p2,p3, ncol = 1)\n",
    "ggsave('histo_transformation.png')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Questions\n",
    "1. Is faceting possible\n",
    "\n",
    "2.  Can we apply proportions to this exploration - what does it tell us? \n",
    "= no -coz its one distribution only. Proprortions Work for 2 or more, for comparative purposes.\n",
    "\n",
    "a Histogram needs a continuous x variable; year and country do not meet this requirement.\n",
    "\n",
    "\n",
    "###Investigate donor aid using a frequency polygon ?\n",
    "This is Not necessary because such a graph serves to compare two or more distributions at once\n",
    "\n",
    "\n",
    "\n",
    "How do you interpret these values?\n",
    "```{r}\n",
    "#Summary stats of donor aid and transformed data\n",
    "\n",
    "summary(aid_received3$donor_aid)\n",
    "\n",
    "summary(log10(aid_received3$donor_aid + 1))\n",
    "\n",
    "summary(sqrt(aid_received3$donor_aid))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "?***qplot(x = year, y = donor_aid, data = aid_received3) + \n",
    "      geom = 'boxplot')\n",
    "\n",
    "### Questions\n",
    "**1.Boxplot ?  how ? \n",
    " do we need to create factor variables?  how?\n",
    "\n",
    "\n",
    "**2. dotplot ?  how ? \n",
    "qplot(x = donor_aid, data= aid_received3, geom_dotplot()) +\n",
    "        scale_x_continuous(lim = c(0,75))\n",
    "-Don't know how to automatically pick scale for object of type -LayerInstance/Layer/ggproto/gg. Defaulting to continuous.\n",
    "-Error: Aesthetics must be either length 1 or the same as the data (5774): x, y\n",
    "\n",
    "**3. Barplot ?  how ? \n",
    "qplot(x = donor_aid, data= aid_received3, geom_bar()) +\n",
    "        scale_x_continuous(lim = c(0,75))\n",
    "        \n",
    "***same prob as above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "or What is the donor_aid range for the middle 50% of 2010?\n",
    "```{r}\n",
    "#How has donor aid changed over the years\n",
    "by(aid_received3$donor_aid, aid_received3$year, summary)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "or What is the donor aid range for the middle 50% for Uganda?\n",
    "```{r}\n",
    "# How has donor aid changed for countries\n",
    "by(aid_received3$donor_aid, aid_received3$country, summary)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "# Interquartile range\n",
    "by(aid_received3$donor_aid, aid_received3$year, IQR)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# Interquartile range\n",
    "by(aid_received3$donor_aid, aid_received3$country, IQR)\n",
    "```\n",
    "\n",
    "\n",
    "Interquartile range\n",
    "```{r}\n",
    "\n",
    "upper_quartile_1960 <- subset(aid_received3, donor_aid >= 10.3 & year == 1960)\n",
    "upper_quartile_1960\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "upper_quartile_2010 <- subset(aid_received3, donor_aid >= 10.3 & year == 2010)\n",
    "upper_quartile_2010\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "\n",
    "max(by(aid_received3$donor_aid, aid_received3$year, IQR))\n",
    "#15.74981\n",
    "\n",
    "min(by(aid_received3$donor_aid, aid_received3$year, IQR))\n",
    "#3.364176\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "max(by(aid_received3$donor_aid, aid_received3$country, IQR))\n",
    "#51.04485\n",
    "#looked up =  Timor-Leste (how to do this progmatically)\n",
    "\n",
    "min(by(aid_received3$donor_aid, aid_received3$country, IQR))\n",
    "# 0.0118714)\n",
    "# manually - libya(how do i do this progmatically?)\n",
    "```\n",
    "\n",
    "### Question\n",
    " if I want a table only of donor aid for the year 1960\n",
    "say - I want to compare it with that of 2010\n",
    "table(aid_received3[aid_received3$year]) == 1960  ???\n",
    "\n",
    "\n",
    "\n",
    "When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).\n",
    "\n",
    "The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Birthdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-5b5ef828566e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-5b5ef828566e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    help(as.Date)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "help(as.Date)\n",
    "help(strptime)\n",
    "\n",
    "Date Formats in R\n",
    "http://www.r-bloggers.com/date-formats-in-r\n",
    "\n",
    "\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson4\")\n",
    "\n",
    "```{r}\n",
    "# installation of necessary tools\n",
    "install.packages('lubridate')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{r }\n",
    "# Readingin and loading  data\n",
    "library(lubridate)\n",
    "\n",
    "birthdays <- read.csv('birthdaysExample.csv')\n",
    "\n",
    "names(birthdays)\n",
    "\n",
    "```\n",
    "```{r}\n",
    "#to extract the birth months and birth days.\n",
    "birthdays$dates <- mdy(birthdays$dates)\n",
    "birthdays$month <- month(birthdays$dates)\n",
    "birthdays$day <- day(birthdays$dates)\n",
    "\n",
    "head(birthdays, 3)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "# Exploring the data\n",
    "summary(birthdays)\n",
    "print('.')\n",
    "\n",
    "dim(birthdays)\n",
    "print('.')\n",
    "\n",
    "str(birthdays)\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# How many people share your birthday (may, 6th) ?\n",
    "\n",
    "may_sixth <- subset(birthdays, month == 05 & day == 06)\n",
    "nrow(may_sixth)\n",
    "\n",
    "```\n",
    "```{r}\n",
    "# Which month contains the most number of birthdays?\n",
    "# march\n",
    "  max(table(birthdays$month))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "#How many birthdays are in each month?\n",
    "table(birthdays$month)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# Which day of the year has the most number of birthdays?\n",
    "table(birthdays$day)\n",
    "max(table(birthdays$day))\n",
    "# 14th\n",
    "```\n",
    "\n",
    "```{r}\n",
    "#Do you have at least 365 friends that have birthdays on everyday of the year?\n",
    "# no(348)\n",
    "  \n",
    "length(unique(birthdays$dates))\n",
    "```\n",
    "\n",
    "```{r}\n",
    "ggplot(birthdays, aes(x=dates)) + \n",
    "  geom_histogram(binwidth = 1, color = \"gray\", fill = \"blue\") + \n",
    "  xlab('Birthday') + ylab('Count') + ggtitle('Histogram of Birthdays')\n",
    "  \n",
    "```{r}\n",
    "ggplot(birthdays, aes(x=month)) + \n",
    "  geom_histogram(binwidth = 1, color = \"gray\", fill = \"blue\") + \n",
    "  scale_x_continuous(lim = c(0, 12)) + \n",
    "  xlab('Birthday') + ylab('Count') + \n",
    "  ggtitle('Histogram of Birthdays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-783e08d37ed3>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-783e08d37ed3>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    ggplot2 geoms\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#lesson 5 Exploring 2 Variables\n",
    "\n",
    "ggplot2 geoms\n",
    "http://docs.ggplot2.org/current/\n",
    "\n",
    "ggplot2 tutorial by Ramon Saccilotto\n",
    "http://bbs.ceb-institute.org/wp-content/uploads/2011/09/handout_ggplot2.pdfLearn more about the dplyr package.\n",
    "\n",
    "Learn more about the dplyr package\n",
    "http://blog.rstudio.org/2014/01/17/introducing-dplyr/\n",
    "\n",
    "Introduction to dplyr (knitted html file)\n",
    "http://rstudio-pubs-static.s3.amazonaws.com/11068_8bc42d6df61341b2bed45e9a9a3bf9f4.html\n",
    "\n",
    "The following tutorials are presented by Hadley Wickham at useR 2014.\n",
    "\n",
    "    Introduction of dplyr - http://www.r-bloggers.com/hadley-wickham-presents-dplyr-at-user-2014/\n",
    "    dplyr Tutorial Part 1 - http://www.r-bloggers.com/hadley-wickhams-dplyr-tutorial-at-user-2014-part-1/\n",
    "    dplyr Tutorial Part 2 - http://www.r-bloggers.com/hadley-wickhams-dplyr-tutorial-at-user-2014-part-2/\n",
    "\n",
    "There are other ways to work with data and create new data frames without using the dplyr package. Learn about the R functions lapply, tapply, and split in a blog post.\n",
    "http://rollingyours.wordpress.com/2014/10/20/the-lapply-command-101/\n",
    "\n",
    "For more on geom_line(), you can check the documentation here.\n",
    "http://docs.ggplot2.org/current/geom_path.html\n",
    "\n",
    "Have questions? Head to the forums for discussion with the Udacity Community.\n",
    "\n",
    "\n",
    "Olive KMG 10/10/2018\n",
    "\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson4\")\n",
    "\n",
    "Lesson 4\n",
    "========================================================\n",
    "\n",
    "***\n",
    "\n",
    "### Scatterplots and Perceived Audience Size\n",
    "Notes:Scatterplots and Perceived Audience Size\n",
    "\n",
    "Notes: We continue with the pseudo-Facebook data. Looking at the Scatterplot for Perceived Audience vs. Actual Audience, we note that if people had guessed correctly their audience size, their point would have landed on the 45 degree line. All points below this x=y line means that their actual audience was greater than their perceived audience. From the graph, we see that this is in fact true (i.e. most data points below this line). All points above the x=y line means that actual audience was less than their perceived audience.\n",
    "\n",
    "One other thing to note is that we see very clear horizontal lines because people were guessing their audience size by either multiples of 5 or 10. There is a very clear horizontal line where Perceived audience = 50 or 100.\n",
    "\n",
    "***\n",
    "\n",
    "### Scatterplots\n",
    "Notes:\n",
    "\n",
    "```{r Scatterplots}\n",
    "#library(\"ggplot2\")\n",
    "#pf <- read.csv('pseudo_facebook.tsv', sep='\\t')\n",
    "\n",
    "ggplot(aes(age, friend_count), data=pf) +\n",
    "  geom_point()\n",
    "\n",
    "or\n",
    "#qplot(x= age, y= friend_count, data= pf)\n",
    "#qplot(age, friend_count, data = pf)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "#### What are some things that you notice right away?\n",
    "Response:\n",
    "What are some things that you notice right away? young users below the age of 30 have a lot of friends.  There vertical bars - are people that have lied about their age, like 69 or 100. These users are likely to be teenagers or fake accounts given the high friend counts.\n",
    "\n",
    "\n",
    "Response: Some people have reported to have a birthday greater than 100, which is most likely to be false. Maybe to stay anonymous. Notice  long right tail data), with exception to those that potentially lied about their age (i.e. age 103 and 108).\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### ggplot Syntax\n",
    "Notes:\n",
    "\n",
    "```{r ggplot Syntax}\n",
    "ggplot(aes(age, friend_count), data=pf) +\n",
    "  geom_point() +  \n",
    "  xlim(13, 90)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Overplotting\n",
    "Notes:\n",
    "Notes: Overlapping points- points are stacked on top of each other - makes it very difficult to tell how many points are in a region. We can set the transparency of the points using the alpha parameter in geom point. Setting alpha=1/20 means that it is going to take 20 points to be the equivalent of one of the original black dots from the previous plot. From this plot, we can see that the bulk of our data lays below the 1000 threshold of friend count.\n",
    "\n",
    "```{r Overplotting}\n",
    "ggplot(aes(age, friend_count), data=pf) +\n",
    "  geom_jitter(alpha=1/20) +  \n",
    "  xlim(13, 90)\n",
    "```\n",
    "\n",
    "#### What do you notice in the plot?\n",
    "Response:\n",
    "We need to add jitter(noise) to our plot. Age is a continous variable so we expect to see a spread of dots not vertical columns or pipes due to only integer values - which is not a true reflection of age. This feels intuitively wrong. So by adding jitter - We can add some noise to this to correct it.  By changing geom_point to geom_jitter.\n",
    "\n",
    "Response: This data appears to be positively skewed (i.e. long right tail). With these changes, our plot is a lot more smooth. We can also see that the friend count for young users are not nearly as high as they use to appear. We still see the peak at 69. This group is comparable to the group in the 25 or 26 age range. Still, the bulk of our data has friend counts below 1000.\n",
    "***\n",
    "\n",
    "### Coord_trans()\n",
    "Notes:\n",
    "Look up the documentation for coord trans() and add a layer to the plot that transforms friend count using the square root function. Create your plot!\n",
    "\n",
    "Notes: Now we want to do a transformation on the y-axis to change the friend counts, so that we can get a better visualization of the data. Here we do a square root coordinate transformation.\n",
    "\n",
    "There are three different ways of doing this (shown below). Here we have to be careful when we use the geom_jitter layer because it will take some of the values where friend count is zero and jitter them to be less than zero. Then when the coord_trans takes the square root of that negative value, it throws an error. So you can either get rid of this error by changing back to a geom_point plot, set the jitter height to be 0, or subset your jittered data to have a friend count greater than 0.\n",
    "```{r Coord_trans()}\n",
    " #Plot with no jitter, using geom_point\n",
    "ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "  geom_point(alpha=1/20) +\n",
    "  xlim(13,90) +\n",
    "  coord_trans(y='sqrt')\n",
    "\n",
    "#or \n",
    "\n",
    "#2. Setting jitter height to 0 (my preferrred method)\n",
    "#ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "#  geom_jitter(alpha=1/20, position=position_jitter(h=0)) +\n",
    "#  xlim(13,90) +\n",
    "#  coord_trans(y='sqrt')\n",
    "\n",
    "```\n",
    "\n",
    "#### ! Look up the documentation for coord_trans() by running this line of code ?coord_trans or visit http://docs.ggplot2.org/current/coord_trans.html.\n",
    "\n",
    "\n",
    "Have questions? Head to the forums for discussion with the Udacity Community.\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "  geom_jitter(alpha=1/20, position=position_jitter(h=0)) +\n",
    "  xlim(13,90) +\n",
    "  coord_trans(y='sqrt')\n",
    "\n",
    "```\n",
    "\n",
    "#### What do you notice? \n",
    "This coordinate transformation allowed us to get a better view of our data that is below the 1000 friend count (i.e. most of our data), while still making sure to contain the rest of our data in the same plot.\n",
    "\n",
    "***\n",
    "\n",
    "### Alpha and Jitter\n",
    "Notes:Notes: Now we use these two new techniques to explore the relationship between friends initiated vs. age. Younger users initiate more friendships than older users, probably because they are expanding their social network at a faster rate than older users due to new experiences such as going to college. There is also a vertical line at 69 years old, but this is probably younger users who thought this age was funny.\n",
    "\n",
    "```{r Alpha and Jitter}\n",
    "ggplot(aes(x=age, y=friendships_initiated), data=pf) +\n",
    "  geom_jitter(alpha=1/25, position=position_jitter(h=0)) +\n",
    "  coord_trans(y='sqrt') +\n",
    "  scale_x_continuous(breaks=seq(10, 90, 5), limits=c(10,90))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### Overplotting and Domain Knowledge\n",
    "Notes:We used alpha and jitter to deal with overplotting. However, there is more that we can do to help resolve this issue, such as using domain knowledge and a transformation to adjust the scatter plot. Turning back to the Scatterplot for Perceived Audience vs. Actual Audience, Moira tranformed both axes. So this time it is a percentage of their friend count. Some people in the study had 50 friends, 100, or 2000, so it makes more sense to think of your audience size as a percentage of the possible audience. Most of the people in the study had set the post to a friends only privacy, so you would expect it to be bounded by their friend count\n",
    "\n",
    "***\n",
    "\n",
    "### Conditional Means\n",
    "Notes:Notes: Our scatter plot is very difficult to analyze because there are so many data points in this one graph. In general, it is not possible to judge important quantities from such a display. Sometimes you want to know how the mean or the median of a variable varies with another variable (ex: conditional means). It can be helpful to summarize the relationship of two variables in other ways instead of plotting every point. For example, we can ask how does the average friend count varies over age. One way to do this is to create a table that, for each age, gives us the mean and median friend count. Let’s use dplyr package.\n",
    "\n",
    "# to get summary tables of a data set we need to use dplyr.\n",
    "group the age variable, then use it to create other new variables using the summarise function.  It also uses  the n() to find out the number of objects in each group.\n",
    "\n",
    "```{r Conditional Means}\n",
    "install.packages('dplyr')\n",
    "library(dplyr)\n",
    "\n",
    "age_groups <- group_by(pf, age) # group by age\n",
    "#summarise the new grouping and create new variables friend_count_mean, #friend_count_median and n\n",
    "pf.fc_by_age <- summarise(age_groups,\n",
    "          friend_count_mean = mean(friend_count),\n",
    "           friend_count_median = median(friend_count),\n",
    "            n = n())\n",
    "\n",
    "#Rearrange the new variable by age\n",
    "pf.fc_by_age <- arrange(pf.fc_by_age,age)\n",
    "\n",
    "head(pf.fc_by_age)\n",
    "          \n",
    "\n",
    "```\n",
    "```{r}\n",
    "#Method 2: Use the pipeline operator from dplyr (read more in the slides provided)\n",
    "pf.fc_by_age <- pf %>% \n",
    "                group_by(age) %>%\n",
    "                summarise(friend_count_mean=mean(friend_count),\n",
    "                friend_count_median=median(friend_count),\n",
    "                 n=n()) %>%\n",
    "                arrange(age)\n",
    "          head(pf.fc_by_age)\n",
    "```\n",
    "\n",
    "\n",
    "Create your plot!\n",
    "\n",
    "\n",
    "```{r Conditional Means Plot}\n",
    "ggplot(aes(x=age, y=friend_count_mean), data=pf.fc_by_age) +\n",
    "  geom_line() +\n",
    "  ggtitle(\"Averge Friend Count vs. Age\") +\n",
    "  labs(x=\"Age\", y=\"Average Friend Count\") +\n",
    "  scale_x_continuous(breaks=seq(11, 116, 5)) +\n",
    "  scale_y_continuous(breaks=seq(60, 500, 50))\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Overlaying Summaries with Raw Data\n",
    "Notes:Notes: ggplot allows us to easily create various summaries of our data and plot them. This can be very useful for quick exploration and for combining plots of raw data, like our original scatter plot with displaying summaries. This newly created overlaid plot immediately reveals the increase in friend count for very young users and the subsequent decrease right after that.\n",
    "\n",
    "##♠Adding Quantile Information\n",
    "\n",
    "To further display how our data is dispursed, we can add the 10th quantile, 50th quantile or median, and the 90th quantile. The mean is the solid black line, the median is the solid blue line, and the 10th and 90th quantile are the dashed blue lines.\n",
    "\n",
    "```{r Overlaying Summaries with Raw Data}\n",
    "#Overlaying the Averge Friend Count vs. Age line onto our data points\n",
    "ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "  geom_point(alpha=.05, \n",
    "              position=position_jitter(h=0),\n",
    "              color='orange') +\n",
    "  xlim(13,90) +\n",
    "  coord_trans(y='sqrt') +\n",
    "  geom_line(stat='summary', fun.y=mean)\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "#Adding in the 10th percentile, 90th percentile, and median to the plot\n",
    "ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "  geom_point(alpha=.05, \n",
    "              position=position_jitter(h=0),\n",
    "              color=\"orange\") +\n",
    "  xlim(13,90) +\n",
    "  coord_trans(y=\"sqrt\") +\n",
    "  geom_line(stat=\"summary\", fun.y=mean) +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.1), \n",
    "            linetype=2, color=\"blue\") +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.9), \n",
    "            linetype=2, color=\"blue\") +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.5),\n",
    "            color=\"blue\") \n",
    "\n",
    "```\n",
    "##Coordinate Cartesian\n",
    "To zoom in, the code should use the coord_cartesian(xlim = c(13, 90)) layer rather than xlim(13, 90) layer. This allows the plot to zoom into the data points that have friend count’s less than 1000, WITHOUT altering the information for the mean, median, and quantile lines.\n",
    "\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(x=age, y=friend_count), data=pf) + \n",
    "  geom_point(alpha=.05, \n",
    "              position=position_jitter(h=0),\n",
    "              color=\"orange\") +\n",
    "  coord_cartesian(xlim=c(13,90), ylim=c(0,1000)) +\n",
    "  geom_line(stat=\"summary\", fun.y=mean) +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.1), \n",
    "            linetype=2, color=\"blue\") +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.9), \n",
    "            linetype=2, color=\"blue\") +\n",
    "  geom_line(stat=\"summary\", fun.y=quantile, fun.args = list(probs=.5),\n",
    "            color=\"blue\")\n",
    "```\n",
    "Note: ggplot 2.0.0 changes the syntax for parameter arguments to functions when using stat = 'summary'. To denote parameters that are being set on the function specified by fun.y, use the fun.args argument, e.g.:\n",
    "\n",
    "ggplot( ... ) +\n",
    "  geom_line(stat = 'summary', fun.y = quantile,\n",
    "            fun.args = list(probs = .9), ... )\n",
    "\n",
    "To zoom in, the code should use thecoord_cartesian(xlim = c(13, 90)) layer rather than xlim(13, 90) layer.\n",
    "\n",
    "Look up documentation for coord_cartesian() and quantile() if you're unfamiliar with them.\n",
    "\n",
    "\n",
    "#### What are some of your observations of the plot?\n",
    "Response:\n",
    "Response: Some of things that are evident from the plot is that there is an increase in friend count for every young users and then it begins to decrease after the age of 20. There is also a very big spike in friend count for those aged 69, which suggest that these might just be users who are a lot younger than 69 years.\n",
    "\n",
    "Similarly, it is very improbable that there are a lot of facebook users that are older than 90. Our data shows that after the 90 age mark, the friend counts begin to increase around the same range as those that are between the age of 13-20, which suggests that these might just be young people lying about their age. The 90th quantile also gives us a very informative upper bound for friend counts per age group. For example, 90% of those that are ~20 years of age have less than ~900 friends.\n",
    "\n",
    "When we zoom in on the plot using the coordinate cartesian layer, we get more detailed information. For example, we can see that for 35 year olds to 60 year olds, the friend count falls below 250. So 90% of this age group has less than 250 friend, according to this facebook dataset.\n",
    "\n",
    "***\n",
    "\n",
    "### Moira: Histogram Summary and Scatterplot\n",
    "See the Instructor Notes of this video to download Moira's paper on perceived audience size and to see the final plot. See the Instructor Notes of this video to download Moira’s paper on perceived audience size and to see the final plot.\n",
    "\n",
    "Notes: For more information on Moira’s exploration of perceived facebook audience, read her paper at: http://hci.stanford.edu/publications/2013/invisibleaudience/invisibleaudience.pdf .\n",
    "\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "### Correlation\n",
    "Notes:\n",
    "A Visual Guide to Correlation\n",
    "https://s3.amazonaws.com/udacity-hosted-downloads/ud651/correlation_images.jpeg\n",
    "\n",
    "Correlation Coefficient\n",
    "http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient\n",
    "\n",
    "Intro to Inferential Statistics- Correlation\n",
    "https://classroom.udacity.com/courses/ud201/lessons/1345848540/concepts/1715827370923\n",
    "\n",
    "Correlation coefficients are often denoted with the greek letter ρ\\rhoρ (rho), in addition to the letter r.\n",
    "\n",
    "The default method for computing the correlation coefficient is Pearson, and this is true for most statistical software. You do not need to pass the method parameter when calculating the Pearson Product Moment Correlation.\n",
    "\n",
    "Have questions? Head to the forums for discussion with the Udacity Community.Look up the documentation for the cor.test function.\n",
    "\n",
    "What's the correlation between age and friend count? Round to three decimal places.\n",
    "Response:\n",
    "\n",
    "```{r Correlation}\n",
    "cor.test(x=pf$age, y=pf$friend_count)#-0.02740737 \n",
    "#same as\n",
    "cor.test(pf$age, pf$friend_count, method = 'pearson')\n",
    "```\n",
    "Therefore there is no meaningfull relationship between these 2 variables\n",
    "\n",
    "```{r}\n",
    "#Alternative method using 'with'\n",
    "with(pf, cor.test(age, friend_count, method=\"pearson\"))\n",
    "```\n",
    "Response: This indicates that there is no meaningful relationship between the 2 variables because r= -0.02740737. \n",
    "##A good rule of thumb is that a correlation greater than 0.3 or less than -0.3 is meaningful, but small. Around 0.5 is moderate and 0.7 or greater is pretty large.\n",
    "\n",
    "***\n",
    "\n",
    "### Correlation on Subsets\n",
    "Notes: Based on the correlation coefficient and our plot, we found that the relationship between age and friend count IS NOT linear. It isn’t monotonic, either increasing or decreasing. Furthermore, based on the plot we know that we don’t want to include the older ages in our correlation number since those oldest ages are likely to be incorrect. Let’s calculate a new r value for those ages less than or equal to 70.\n",
    "\n",
    "```{r Correlation on Subsets}\n",
    "with(subset(pf, age <= 70), cor.test(age, friend_count))\n",
    " \n",
    "#Or\n",
    "\n",
    "with(pf[pf$age <=70,], cor.test(age, friend_count))\n",
    "```\n",
    "This gives us a different summary statistic, r = -0.1717245. This tells a different story about a negative relationship between age and friend count. As age increases, we can see that friend count decreases.\n",
    "\n",
    "It is IMPORTANT to note that one variable DOES NOT cause the other. For example, it would be unwise to say that growing old means that you have fewer internet friends. We would really need to have data from experimental research and make use of Inferential Statistics, rather than Descriptive Statistics to address causality.\n",
    "***\n",
    "\n",
    "### Correlation Methods\n",
    "http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/\n",
    "\n",
    "Notes:Notes: The Pearson Product-Moment correlation measures the strength of relationship between any two variables, but there can be lots of other types of relationships. Even other ones that are monotonic (either increasing or decreasing).\n",
    "\n",
    "So we also have measures of monotonic relationships, such as a rank correlation measures like Spearman. We can assign Spearman to the method parameter and calculate the correlation that way. Here our test statistic is “rho” with a different value than we previously had obtained with “spearman”.\n",
    "\n",
    "Read more here: http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/ “Spearman rank correlation is a non-parametric test that is used to measure the degree of association between two variables. It was developed by Spearman, thus it is called the Spearman rank correlation. Spearman rank correlation test does not assume any assumptions about the distribution of the data and is the appropriate correlation analysis when the variables are measured on a scale that is at least ordinal.”\n",
    "```{r}\n",
    "with(pf[pf$age <=70,], cor.test(age, friend_count, method=\"spearman\"))\n",
    "```\n",
    "One key thing to note out of these various methods is that single number coefficients like this are useful, but they are not a great substitue for looking at a scatter plot and computing conditional summaries like we did earlier. We get a much richer understanding by incorporating visualization methods.\n",
    "\n",
    "Correlation Methods: Pearson's r, Spearman's rho, and Kendall's tau\n",
    "http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/\n",
    "***\n",
    "\n",
    "## Create Scatterplots\n",
    "Notes:Let’s continue our analysis with examples of variables that are more highly correlated. We will look at the likes users received from the desktop version of the site (www likes received) and the total number of likes users received (likes received). Let’s ignore the mobile likes for now.\n",
    "\n",
    "# Create a scatterplot of likes_received (y)\n",
    "# vs. www_likes_received (x). Use any of the\n",
    "# techniques that you've learned so far to\n",
    "# modify the plot.\n",
    "```{r}\n",
    "\n",
    "ggplot(aes(x=www_likes_received, y=likes_received), data=pf) +\n",
    "  geom_point(alpha = 0.1) +\n",
    "  ggtitle(\"likes_received vs. www_likes_received\") +\n",
    "  coord_trans(x=\"sqrt\", y='sqrt')\n",
    "```\n",
    "#Looking at the original plot, lets limit x=c(0,12500) and y=c(0, 25000) since the bulk of our data is between this range\n",
    "***\n",
    "\n",
    "### Strong Correlations\n",
    "Notes:Notes: Looking at our above plots, we see that there are multiple outliers in here. And the bulk of our data is towards the bottom. To figure our some good bonds for our axis, we can use the quantile function to specify the 95th quantile as our upper bound for our x-axis and also our y-axis. Let’s  add a smoother and set the method to linear model or lm, color it red for visibility)\n",
    "# 0 is the lowerbound and 95% the upper bound of our data(zooming in)\n",
    "best fit line, which is the slope = coorelation\n",
    "\n",
    "```{r Strong Correlations}\n",
    "ggplot(aes(x=www_likes_received, y=likes_received), data=pf) +\n",
    "  geom_point(alpha=.1) +\n",
    "  xlim(0, quantile(pf$www_likes_received, 0.95)) +\n",
    "  ylim(0, quantile(pf$likes_received, 0.95)) +\n",
    "  geom_smooth(method=\"lm\", color=\"red\")\n",
    "\n",
    "```\n",
    "\n",
    "#Lets quantify this relationship\n",
    "What's the correlation betwen the two variables? Include the top 5% of values for the variable in the calculation and round to 3 decimal places. i.e include all the data.\n",
    "...no subsets\n",
    "\n",
    "```{r Correlation Calcuation}\n",
    "with(pf, cor.test(www_likes_received, likes_received))\n",
    "\n",
    "#or\n",
    "\n",
    "#cor.test(pf$(www_likes_received, pf$likes_received))\n",
    "\n",
    "```\n",
    "\n",
    "Response:We found a very strong correlation, r = 0.9479902! However, oftentimes our values are not that strongly correlated. This high r value we found was an artifact of the nature of the variables. One of them was a superset of the o\n",
    "\n",
    "***\n",
    "\n",
    "### Moira on Correlation\n",
    "Notes:Strong correlations might not always be a good thing. Let’s hear from Moira about why we look at correlation in the first place and what it can tell us about two variables.\n",
    "\n",
    "Moira talks about how a lot of the data she works with in Facebook is typically highly correlated with each other, so it is important to quantify the degree of relatedness. For example, how many times someone logs on, how many times they post a day, or how many likes they get ....are usually highly correlated and that is because they typically MEASURE THE SAME THING (i.e. how engaged someone is with the website). So, typically when she is working with a problem and doing some kind of regression, where she is going to be modeling something, she is going to be throwing only some of these variables into the regression. \n",
    "Note that - One of the assumptions of regression is these variables are independent of each other. So if any two are highly correlated with each other, it will be very difficult to tell which ones are actually driving the phenomenon. And so it’s important to measure the correlation between your variables first, often because it will help you determine which ones you don’t actually want to throw in together, and it might help you decide which oes you actually want to keep.\n",
    "\n",
    "***\n",
    "\n",
    "### More Caution with Correlation\n",
    "Notes:Notes: As Moira discussed, correlation coefficents can help us decide how related two variables are. However, if we are not careful, the r value can be deceptive if you are not careful. Plotting data can be helpful to get key insights from our data.\n",
    "\n",
    "Let’s look at another data set that comes with the alr3 package. We will look at the Mitchell dataset, which contains soil temperatures from Mitchell, Nebraska. By looking at this dataset, we will see how correlation can be somewhat deceptive.\n",
    "\n",
    "```{r More Caution With Correlation}\n",
    "#install.packages('alr3')\n",
    "#library(alr3)\n",
    "head(Mitchell)\n",
    "```\n",
    "\n",
    "\n",
    "Create your plot!\n",
    "\n",
    "```{r Temp vs Month}\n",
    "library(ggplot2)\n",
    "ggplot(aes(Month, Temp), data=Mitchell) +\n",
    "  geom_point() +\n",
    "  ggtitle(\"Mitchell Soil: Temp vs. Month\")\n",
    "```\n",
    "Argument matching (when not providing them by name) in R is a bit complex.\n",
    "\n",
    "First, arguments (or parameters) can be matched by name. If a parameter matches exactly, it is \"removed\" from the argument list and the remaining unnamed arguments are matched in the order that they are listed in the function definition.\n",
    "\n",
    "R does the following to match arguments...\n",
    "\n",
    "    checks for exact match of named argument\n",
    "    checks for a partial match of the argument\n",
    "    checks for a positional match\n",
    "\n",
    "If R does not find a match for a parameter, it typically throws an \"unused\" parameter error.\n",
    "\n",
    "Type str(functionName) to find the order of the parameters and learn more about the parameters of an R function. \n",
    "***\n",
    "\n",
    "### Noisy Scatterplots\n",
    "a. Take a guess for the correlation coefficient for the scatterplot.\n",
    "\n",
    "b. What is the actual correlation of the two variables?\n",
    "(Round to the thousandths place)\n",
    "\n",
    "```{r Noisy Scatterplots}\n",
    "\n",
    "with(Mitchell, cor.test(Month, Temp))\n",
    "\n",
    "#or\n",
    "\n",
    "cor.test(mitchell$Month, mitchell$Temp)\n",
    "```\n",
    "pretty weak coorelation\n",
    "***\n",
    "\n",
    "### Making Sense of Data\n",
    "Notes:\n",
    "range(Mitchell$Month)  = 0 203\n",
    "\n",
    "```{r Making Sense of Data}\n",
    "ggplot(aes(Month, Temp), data=Mitchell) +\n",
    "  geom_point() +\n",
    "  ggtitle(\"Mitchell Soil: Temp vs. Month\") +\n",
    "    scale_x_continuous(breaks=seq(0, 204, 12))\n",
    "```\n",
    "Better plot\n",
    "***\n",
    "\n",
    "### A New Perspective -  pay attention to the context of the data\n",
    "\n",
    "What do you notice?What do you notice? Response: Stretching out our new plot, we see a cyclical pattern for every year interval.\n",
    "\n",
    "Notes: The cor and cor.test functions determine the strength of a linear relationship, but they may miss other relationships in the data. It looks like a sine or cosine graph, which makes sense based on what the data is telling us because of the seasons. It is important to pay attention to the context of the data. Also, it is imporant to note that the proportion of the scale of the graphics do matter. Pioneers in the field of data visualization, such as Playfair and Tukey studied this extensively (http://www.psych.utoronto.ca/users/spence/Spence%20(2006).pdf).\n",
    "\n",
    "#They determined that the nature of the data should suggest the shape of the graphic. Otherwise, you should tend to have a graphic that’s about 50% wider than it is tall.\n",
    "\n",
    "#Overlaying Time Series to Display pattern\n",
    "You could also get perspective on this data by overlaying each year’s data on top of each other, giving a clear, generally sinusoidal graph. You can do this by using the R’s modulus operator %% in the code.\n",
    "Response:\n",
    "\n",
    "Watch the solution video and check out the Instructor Notes!\n",
    "Notes:\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(Month%%12, Temp), data=Mitchell) +\n",
    "  geom_point() +\n",
    "  ggtitle(\"Overlayed Mitchell Soil Data: Temp vs. Month\")\n",
    "```\n",
    "\n",
    "***\n",
    "Simulated Data: Using dcor.ttest\n",
    "\n",
    "There are other measures of associations that can detect these types of patterns. The dcor.ttest function in the energy package implements a non-parametric test of the independence of two variables. While the Mitchell soil dataset is too coarse to identify a significant dependency between “Month” and “Temp”, we can see the difference between dcor.ttest and cor.test through other examples, like the following:\n",
    "```{r}\n",
    "x <- seq(0, 4*pi, pi/20)\n",
    "y <- cos(x)\n",
    "qplot(x = x, y = y)\n",
    "```\n",
    "```{r}\n",
    "cor.test(x,y)\n",
    "```\n",
    "```{r}\n",
    "#install.packages(\"energy\")\n",
    "suppressMessages(library(\"energy\"))\n",
    "dcor.ttest(x, y) \n",
    "```\n",
    "Notice that the r is a very tiny negative number close to 0 and misses the sinusoidal relationship. The dcor does a t-test of independence. The p-value = 0.025, so reject null hypothesis that both variables are independent.\n",
    "\n",
    "\n",
    "### Understanding Noise: Age to Age Months\n",
    "Notes: From our previous plot of friend count mean vs. age, we can see that the y-axis has a lot of noise. Looking at the rows 17-19, we can see that the 30 year olds have a lower mean friend count than the 29 or 31 year olds. Some year to year discontinuities might make sense, like age 69, but others are probably just noise around the smoother relationship between age and friend count. This shows that we only have a sample from the population and our data represents the true mean plus some noise for each year. The noise for this plot would have been worst if we chose finer bins for age. For example, we can estimate conditional means for each age, measured in months instead of years.\n",
    "\n",
    "```{r Understanding Noise: Age to Age Months}\n",
    "\n",
    "pf$age_with_months <- with(pf, age + (1 - dob_month / 12))\n",
    "\n",
    "names(pf)\n",
    "```\n",
    "\n",
    "***\n",
    "### Age with Months Means# Create a new data frame called\n",
    "# pf.fc_by_age_months that contains\n",
    "# the mean friend count, the median friend\n",
    "# count, and the number of users in each\n",
    "# group of age_with_months. The rows of the\n",
    "# data framed should be arranged in increasing\n",
    "# order by the age_with_months variable.\n",
    "\n",
    "# For example, the first two rows of the resulting\n",
    "# data frame would look something like...\n",
    "\n",
    "# age_with_months  friend_count_mean    friend_count_median n\n",
    "#              13            275.0000                   275 2\n",
    "#        13.25000            133.2000                   101 11\n",
    "\n",
    "\n",
    "\n",
    "head(pf.fc_by_age_months)\n",
    "\n",
    "#With Means\n",
    "#Hint 1: Use the group_by(), summarise(), and arrange() functions in the dplyr package to split the data frame by age_with_month. Make sure you arrange by the correct variable (it's not age anymore).\n",
    "\n",
    "\n",
    "\n",
    "#Hint 2: The code should look similar to the code when we split the data frame by age and found summaries:\n",
    "\n",
    "age_groups <- group_by(pf, age)\n",
    "pf.fc_by_age <- summarise(age_groups,\n",
    "                          friend_count_mean = mean(friend_count),\n",
    "                          friend_count_median = median(friend_count),\n",
    "                          n = n())\n",
    "pf.fc_by_age <- arrange(pf.fc_by_age, age)\n",
    "head(pf.fc_by_age)\n",
    "\n",
    "\n",
    "```{r Age with Months Means}\n",
    "#install.packages(\"dplyr\")\n",
    "library(dplyr)\n",
    "\n",
    "pf.fc_by_age_months <- pf %>% \n",
    "  group_by(age_with_months) %>%\n",
    "  summarise(friend_count_mean=mean(friend_count),\n",
    "            friend_count_median=median(friend_count),\n",
    "            n=n()) %>%\n",
    "  arrange(age_with_months) \n",
    "\n",
    "head(pf.fc_by_age_months)\n",
    "```\n",
    "\n",
    "Programming Assignment\n",
    "```{r Programming Assignment}\n",
    "\n",
    "#Method 2: break up the commands instead of using pipeline notation\n",
    "#age_with_months_groups <- group_by(pf, age_with_months)\n",
    "#pf.fc_by_age_months2 <- summarise(age_with_months_groups,\n",
    "#                                  friend_count_mean=mean(friend_count),\n",
    "#                                  friend_count_median=median(friend_count),\n",
    "#                                  n=n())\n",
    "#pf.fc_by_age_months2 <- arrange(pf.fc_by_age_months2, age_with_months)\n",
    "#head(pf.fc_by_age_months2)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Noise in Conditional Means\n",
    "Now we have our data frame with our conditional means measured in months. Now, we plot.\n",
    "# Create a new scatterplot showing friend_count_mean\n",
    "# versus the new variable, age_with_months. Be sure to use\n",
    "# the correct data frame (the one you create in the last\n",
    "# exercise) AND subset the data to investigate\n",
    "# users with ages less than 71.\n",
    "\n",
    "\n",
    "```{r Noise in Conditional Means}\n",
    "library(ggplot2)\n",
    "ggplot(subset(pf.fc_by_age_months, age_with_months<71), aes(x=age_with_months, y=friend_count_mean)) + geom_line()\n",
    "\n",
    "#or\n",
    "#ggplot(aes(age_with_months, friend_count_mean),\n",
    "#       data = pf.fc_by_age_months[pf.fc_by_age_months$age_with_months < #71,] ) +\n",
    "#  geom_line()\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Smoothing Conditional Means\n",
    "Notes:Notes: Let’s compare our two plots that we have created thus far using the regular age variable and the age with months variable. Let’s plot them on the same screen like we did before using grid.arrange.\n",
    "\n",
    "```{r Smoothing Conditional Means}\n",
    "p1 <- ggplot(aes(age, friend_count_mean),\n",
    "       data=subset(pf.fc_by_age, age < 71)) +\n",
    "  geom_line()\n",
    "\n",
    "p2 <- ggplot(aes(age_with_months, friend_count_mean),\n",
    "       data=subset(pf.fc_by_age_months, age_with_months < 71)) +\n",
    "  geom_line()\n",
    "\n",
    "suppressMessages(library(gridExtra))\n",
    "grid.arrange(p2, p1, ncol=1)\n",
    "\n",
    "```\n",
    "So we see the difference between age and age with months. By decreasing the size of our bins and increasing the number of bins, we have less data to estimate each conditional mean. The noise is worse on the top graph since we have finer bin choices.\n",
    "\n",
    "On the other hand, we can go the other direction and increase the size of the bins. Say, we could lump everyone together whose age falls on a multiple of 5. Essentially, what we will do is that we would cut our graph in pieces and average these mean friend counts together. So, users who are within two and a half years of 40 will get lumpted into one point. The same will be true for users who are two and a half years of 50.\n",
    "\n",
    "In the code, we are creating a plot with age that’s been divided by five, rounded, and then multiplied by 5. We then add a geom line. We DON’T want to plot the friend count, but rather the mean friend count.\n",
    "***\n",
    "\n",
    "### Which Plot to Choose?\n",
    "Notes:\n",
    "Local Regression (LOESS) explained visually on the Simply Statistics blog.http://simplystatistics.org/2014/02/13/loess-explained-in-a-gif/\n",
    "http://simplystatistics.org/\n",
    "\n",
    "The Details of Loess and Lowess\n",
    "http://en.wikipedia.org/wiki/Local_regression\n",
    "\n",
    "```{r}\n",
    "p3 <- ggplot(aes(x = round(age/5)*5, y = friend_count),\n",
    "             data = subset(pf, age < 71)) +\n",
    "  geom_line(stat=\"summary\", fun.y=mean)\n",
    "grid.arrange(p2,p1,p3, ncol=1)\n",
    "\n",
    "```\n",
    "See how we have less data points? And wider bin widths? By doing this, we would estimate the mean more precisely, but potentially miss important features of the age and friend count relationship. These 3 plots are an example of the Bias-Variance Tradeoff! It is similar to the tradeoff we make when choosing the bin width in a histogram. One better way that analyst can better make this tradeoff is by using a flexible statistical model to smooth our estimates of conditional means.\n",
    "\n",
    "ggplot makes it easier to fit such models using geom_smooth. So, instead of seeing all the noise (top plot), we’ll have a smooth modular function that will fit along the data. We will do the same for the second plot as well. Let’s add the geom_smooth layer to the first and second plot. We are using the default ggplot, so all the decisions about what model we are using will be made for us. Read more on the geom smooth documentation to learn more about different models.\n",
    "\n",
    "```{r}\n",
    "p1 <- ggplot(aes(age, friend_count_mean),\n",
    "       data=subset(pf.fc_by_age, age < 71)) +\n",
    "  geom_line() +\n",
    "  geom_smooth()\n",
    "\n",
    "p2 <- ggplot(aes(age_with_months, friend_count_mean),\n",
    "       data=subset(pf.fc_by_age_months, age_with_months < 71)) +\n",
    "  geom_line() +\n",
    "  geom_smooth()\n",
    "\n",
    "p3 <- ggplot(aes(x = round(age/5)*5, y = friend_count),\n",
    "             data = subset(pf, age < 71)) +\n",
    "  geom_line(stat=\"summary\", fun.y=mean)\n",
    "grid.arrange(p2,p1,p3, ncol=1)\n",
    "```\n",
    "\n",
    "So now we see the smoother for age with months and the smoother for age. While the smoother captures some of the features of the relationship, it doesn’t draw attention to the the non-motonic relationship in the low ages well. It really misses the discontinuity at age 69. This shows that using models like LOWESS (local regression) or SMOOTHING SPLINES can be useful, but like nearly any model, it can be subject to systematic errors, when the true process generating our data isn’t so consistent with the model itself. Here, the models are based on the idea that the true functions are smooth, but we really know that there are some discontinuities in the relationship. Resources: https://en.wikipedia.org/wiki/Local_regression http://simplystatistics.org/2014/02/13/loess-explained-in-a-gif/\n",
    "Which Plot to Choose?\n",
    "\n",
    "#Notes: \n",
    "So far, we have looked at lots of different plots of the same data and talking about some of the trade offs in data visualization. So which plot is the one to choose? We don’t have to choose! In data visualization, it is important to look at these various plots and numeric summaries and extract important information from these methods. When refining a plot, the latest version is not necessarily better than the first version. They might just show different qualitities of our data. However, when it it time to present to an audience, we might have to choose 1 or 2 visualizations that best communicate the findings of your work.\n",
    "\n",
    "##Analyzing Two Variables\n",
    "\n",
    "#Reflection: \n",
    "In this lesson, we covered scatterplots, conditional means, and correlation coeffients. Reflect on what you have learned and submit your ideas. As mentioned before, I already have a Bachelors in Statistics and have worked a lot with R. However, this course has allowed me to think at a much deeper level about data visualization. ggplot is an AMAZING graphical package that helps me explore my data with ease. I really enjoyed the overlaying techniques, specifically learning how to deal with overplotting. The tidyr and the dplyr package facilitate the way that you work with data frames, while keeping the syntax very simple.\n",
    "\n",
    "Final Video Remarks: In this lesson, we learned how to explore the relationship between two variables using scatterplots. We also augmented this plot with conditional summaries like means. We also learned about the benefits and the limitations of using correlation to understand the relationship between two variables and how correlation may affect your decision over which variables to include in your final model.\n",
    "\n",
    "We also learned how to adjust our data visualization techniques and not trust our interpretation of initial scatterplots, like with the seasonal temperature data. And we learned how to use jitter and transparency to reduce overplotting.\n",
    "\n",
    "A deep dive into Bivariate Data Analysis.\n",
    "http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-bivariate-slides.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-0eaeee97406c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-0eaeee97406c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Save save save save save***1000000000000000000000000000\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Save save save save save***1000000000000000000000000000\n",
    "Problem set: Explore Two Variables\n",
    "Lesson 6: Problem Set - Explore Two Variables\n",
    "\n",
    "1. Create a scatterplot of price vs x, using the ggplot syntax.\n",
    "\n",
    "\n",
    "```{r}\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "data(diamonds)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(x=x, y=price), data = diamonds) + \n",
    "  geom_point(alpha= 1/10, position = position_jitter(h = 0)) + \n",
    "  coord_cartesian(xlim = c(3, 10))\n",
    "```\n",
    "\n",
    "#Correlations \n",
    "What is the correlation between price and x?\n",
    "What is the correlation between price and y?\n",
    "What is the correlation between price and z?\n",
    "between price and x,y,z.\n",
    "\n",
    "```{r}\n",
    "cor.test(diamonds$price, diamonds$x) \n",
    "#or\n",
    "with(diamonds, cor.test(x=price, y = x, method = 'pearson'))\n",
    "```\n",
    "\n",
    "cor.test(diamonds$price, diamonds$y) \n",
    "#or\n",
    "with(diamonds, cor.test(x=price, y = y, method = 'pearson'))\n",
    "\n",
    "cor.test(diamonds$price, diamonds$z) \n",
    "#or\n",
    "with(diamonds, cor.test(x=price, y = z, method = 'pearson'))\n",
    "\n",
    "## \n",
    "##  Pearson's product-moment correlation\n",
    "## \n",
    "## data:  diamonds$price and diamonds$x\n",
    "## t = 440.16, df = 53938, p-value < 2.2e-16\n",
    "## alternative hypothesis: true correlation is not equal to 0\n",
    "## 95 percent confidence interval:\n",
    "##  0.8825835 0.8862594\n",
    "## sample estimates:\n",
    "##       cor \n",
    "## 0.8844352\n",
    "\n",
    "cor.test(diamonds$price, diamonds$y)\n",
    "\n",
    "## \n",
    "##  Pearson's product-moment correlation\n",
    "## \n",
    "## data:  diamonds$price and diamonds$y\n",
    "## t = 401.14, df = 53938, p-value < 2.2e-16\n",
    "## alternative hypothesis: true correlation is not equal to 0\n",
    "## 95 percent confidence interval:\n",
    "##  0.8632867 0.8675241\n",
    "## sample estimates:\n",
    "##       cor \n",
    "## 0.8654209\n",
    "\n",
    "cor.test(diamonds$price, diamonds$z)\n",
    "\n",
    "## \n",
    "##  Pearson's product-moment correlation\n",
    "## \n",
    "## data:  diamonds$price and diamonds$z\n",
    "## t = 393.6, df = 53938, p-value < 2.2e-16\n",
    "## alternative hypothesis: true correlation is not equal to 0\n",
    "## 95 percent confidence interval:\n",
    "##  0.8590541 0.8634131\n",
    "## sample estimates:\n",
    "##       cor \n",
    "## 0.8612494\n",
    "\n",
    "#Quiz:Create a simple scatter plot of price vs depth.\n",
    "```{r}\n",
    "ggplot(aes(x=depth, y=price), data=diamonds) + geom_point()\n",
    "```\n",
    "\n",
    "#adjustments\n",
    "Change the code to make the transparency of the points to be 1/100 of what they are now and mark the x-axis every 2 units. See the instructor notes for two hints.\n",
    "```{r}\n",
    "ggplot(data = diamonds, aes(x = depth, y = price)) +\n",
    "  geom_point(alpha = 1/100) +\n",
    "  scale_x_continuous(breaks = seq(0,80,2))\n",
    "```\n",
    "\n",
    "#Typical depth range\n",
    "60 - 63\n",
    "\n",
    "Correlation between depth and price.\n",
    "\n",
    "cor.test(diamonds$depth, diamonds$price)\n",
    "\n",
    "## \n",
    "##  Pearson's product-moment correlation\n",
    "## \n",
    "## data:  diamonds$depth and diamonds$price\n",
    "## t = -2.473, df = 53938, p-value = 0.0134\n",
    "## alternative hypothesis: true correlation is not equal to 0\n",
    "## 95 percent confidence interval:\n",
    "##  -0.019084756 -0.002208537\n",
    "## sample estimates:\n",
    "##        cor \n",
    "## -0.0106474\n",
    "\n",
    "#Create a scatterplot of price vs carat and omit the top 1% of price and carat values.\n",
    "\n",
    "```{r}\n",
    "ggplot(data = diamonds, aes(x = carat, y = price)) +\n",
    "  geom_point() +\n",
    "  xlim(0, quantile(diamonds$carat, 0.99)) +\n",
    "  ylim(0, quantile(diamonds$price, 0.99))\n",
    "```\n",
    "\n",
    "# price vs. volume\n",
    "# Create a scatterplot of price vs. volume (x * y * z).\n",
    "# This is a very rough approximation for a diamond's volume.\n",
    "\n",
    "# Create a new variable for volume in the diamonds data frame.\n",
    "# This will be useful in a later exercise.\n",
    "\n",
    "# Don't make any adjustments to the plot just yet.\n",
    "\n",
    "```{r}\n",
    "diamonds$volume <- diamonds$x*diamonds$y*diamonds$z\n",
    "\n",
    "head(diamonds,2)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "ggplot(data = diamonds, aes(x = volume, y = price)) +\n",
    "  geom_point()\n",
    "```\n",
    "\n",
    "#observations\n",
    "-a few  outliers. \n",
    "-Some volumes are 0! \n",
    "-There's an expensive diamond with a volume near 4000, and a cheaper diamond with a volume near 900.\n",
    "\n",
    "You can find out how many diamonds have 0 volume by using count(diamonds$volume == 0). The count() function comes with the plyr package.\n",
    "\n",
    "Note: If you ran the count function from plyr, you need to run this command in R to unload the plyr package.\n",
    "detach(\"package:plyr\", unload=TRUE)\n",
    "The plyr package will conflict with the dplyr package in later exercises.\n",
    "\n",
    "Depending on your investigation, it may or may not be important for you to understand how outliers, like these, came to be in your data.\n",
    "\n",
    "```{r}\n",
    "library(plyr)\n",
    "\n",
    "\n",
    "# Some outliers with zero volume\n",
    "count(diamonds$volume == 0)\n",
    "```\n",
    "\n",
    "\n",
    "detach(\"package:plyr\", unload=TRUE) #creates an error\n",
    "\n",
    "#Correlations on Subsets\n",
    "What is the correlation or price and volume?\n",
    "#Hint 1:\n",
    "It’s helpful to add a variable for volume to the diamonds data frame. You should exclude diamonds that have a volume greater than or equal to 800. Also, exclude diamonds that have a volume of 0. Use the & symbol between two conditions when you subset the original data frame.\n",
    "#Hint 2:\n",
    "Subset the diamonds data frame under the conditions and save it into another variable. Use that data frame in the cor() function\n",
    "```{r}\n",
    "volume_sub <- subset(diamonds, volume != 0 & volume < 800)\n",
    "\n",
    "cor(volume_sub$price, volume_sub$volume)\n",
    "```\n",
    "\n",
    "#Adjustments - price vs. volume\n",
    "# Subset the data to exclude diamonds with a volume\n",
    "# greater than or equal to 800. Also, exclude diamonds\n",
    "# with a volume of 0. Adjust the transparency of the\n",
    "# points and add a linear model to the plot. (See the\n",
    "# Instructor Notes or look up the documentation of\n",
    "# geom_smooth() for more details about smoothers.)\n",
    "\n",
    "# We encourage you to think about this next question and\n",
    "# to post your thoughts in the discussion section.\n",
    "\n",
    "# Do you think this would be a useful model to estimate\n",
    "# the price of diamonds? Why or why not?\n",
    "\n",
    "Smoothing :\n",
    "http://www.ats.ucla.edu/stat/r/faq/smooths.htm\n",
    "\n",
    "Subset the data to exclude diamonds with a volume greater than or equal to 800. Also, exclude diamonds with a volume of 0. Adjust the transparency of the points and add a linear model to the plot.\n",
    "```{r}\n",
    "ggplot(data = volume_sub, aes(x = volume, y = price)) +\n",
    "  geom_point(alpha = 1/50) +\n",
    "  xlim(0,500) +\n",
    "  geom_smooth()\n",
    "```\n",
    "Types of smoothers in ggplot2.\n",
    "https://stats.idre.ucla.edu/r/faq/how-can-i-explore-different-smooths-in-ggplot2/\n",
    "\n",
    "Yes - its a fine fit for the data therefor representative\n",
    "\n",
    "```{r}\n",
    "\n",
    "library(gridExtra)\n",
    "\n",
    "p1 <- ggplot(data = subset(diamonds, (volume > 0) & (volume <= 800)),\n",
    "       aes(x = volume, y = price)) +\n",
    "  geom_point() \n",
    "\n",
    "# Default smoother\n",
    "p2 <- p1 + geom_smooth()\n",
    "\n",
    "# looking at a linear fit,\n",
    "p3 <- p1 + stat_smooth(method = \"lm\", formula = y ~ x, size = 1) + coord_cartesian(ylim = c(0,20000))\n",
    "\n",
    "# Looking at polynimoal functions of order 2\n",
    "p4 <- p1 + stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), size = 1) + coord_cartesian(ylim = c(0,20000))\n",
    "\n",
    "# Looking at polynimoal functions of order 3\n",
    "p5 <- p1 + stat_smooth(method = \"lm\", formula = y ~ poly(x, 3), size = 1) + coord_cartesian(ylim = c(0,20000))\n",
    "\n",
    "grid.arrange(p2,p3,p4,p5,ncol =2)\n",
    "```\n",
    "\n",
    "\n",
    "#Mean Price by Clarity\n",
    "# Use the function dplyr package\n",
    "# to create a new data frame containing\n",
    "# info on diamonds by clarity.\n",
    "\n",
    "# Name the data frame diamondsByClarity\n",
    "\n",
    "# The data frame should contain the following\n",
    "# variables in this order.\n",
    "\n",
    "#       (1) mean_price\n",
    "#       (2) median_price\n",
    "#       (3) min_price\n",
    "#       (4) max_price\n",
    "#       (5) n\n",
    "\n",
    "# where n is the number of diamonds in each\n",
    "# level of clarity.\n",
    "\n",
    "```{r}\n",
    "library(dplyr)\n",
    "detach(\"package:plyr\", unload=TRUE)\n",
    "\n",
    "clarity_groups <- group_by(diamonds, clarity)\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "diamondsByClarity <- diamonds %>%\n",
    "      group_by(clarity) %>%\n",
    "      summarize(\n",
    "            mean_price = mean(as.numeric(price)),\n",
    "            median_price = median(as.numeric(price)),\n",
    "            min_price = min(as.numeric(price)),\n",
    "            max_price = max(as.numeric(price)),\n",
    "            n = n())\n",
    "\n",
    "diamondsByClarity\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "diamondsbyClarity <- summarise(clarity_groups,\n",
    "                               mean_price = mean(price), \n",
    "                               median_price = median(price), \n",
    "                               min_price = min(price), \n",
    "                               max_price = max(price), \n",
    "                               n = n())\n",
    "\n",
    "diamondsbyClarity\n",
    "```\n",
    "\n",
    "#Bar Charts of Mean Price\n",
    "BARCHARTS http://docs.ggplot2.org/0.9.3/geom_bar.html\n",
    "DIFFERENCE BETWEEN BARCHART AND HISTOGRAM http://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/\n",
    "\n",
    "\n",
    "Write additional code to create three bar plots on one output image using the grid.arrange() function from the package gridExtra.\n",
    "\n",
    "```{r}\n",
    "diamonds_by_clarity <- group_by(diamonds, clarity)\n",
    "diamonds_mp_by_clarity <- summarise(diamonds_by_clarity, mean_price = mean(price))\n",
    "\n",
    "diamonds_by_color <- group_by(diamonds, color)\n",
    "diamonds_mp_by_color <- summarise(diamonds_by_color, mean_price = mean(price))\n",
    "\n",
    "p1  <- ggplot(diamonds_mp_by_clarity, aes(x = clarity, y = mean_price, fill= clarity)) +\n",
    "  geom_bar(stat = \"identity\")\n",
    "\n",
    "p2 <- ggplot(diamonds_mp_by_color, aes(x = color, y = mean_price, fill= color)) +\n",
    "  geom_bar(stat = \"identity\")\n",
    "\n",
    "grid.arrange(p1,p2, ncol =2)\n",
    "```\n",
    "#observation\n",
    "These trends seem to go against our intuition. Mean price tends to decrease as clarity improves. The same can be said for color.\n",
    "\n",
    "#Alternative\n",
    "```{r}\n",
    "diamonds_by_clarity <- group_by(diamonds, clarity)\n",
    "diamonds_mp_by_clarity <- summarise(diamonds_by_clarity, mean_price = mean(price))\n",
    "\n",
    "diamonds_by_color <- group_by(diamonds, color)\n",
    "diamonds_mp_by_color <- summarise(diamonds_by_color, mean_price = mean(price))\n",
    "\n",
    "diamonds_by_cut <- group_by(diamonds, cut)\n",
    "diamonds_mp_by_cut <- summarise(diamonds_by_cut, mean_price = mean(price))\n",
    "\n",
    "p1 <- ggplot(data = diamonds_mp_by_clarity,\n",
    "aes(clarity,mean_price)) + \n",
    "geom_bar(stat = \"identity\")\n",
    "\n",
    "p2 <- ggplot(data = diamonds_mp_by_color, aes(color,mean_price)) +\n",
    "geom_bar(stat = \"identity\")\n",
    "\n",
    "p3<- ggplot(data = diamonds_mp_by_cut, aes(cut, mean_price)) + geom_bar(stat = \"identity\")\n",
    "# combine\n",
    "\n",
    "grid.arrange(p1, p2 , p3, ncol = 3)\n",
    "\n",
    "```\n",
    "\n",
    "# combine\n",
    "grid.arrange(p1, p2 , p3, ncol = 3)\n",
    "\n",
    "Gapminder Work follows below:\n",
    "##Selected - Average age of dollar billionaires (years)|data from Forbes|Economy -Poverty & inequality##\n",
    "\n",
    "setwd(\"~/data-analytics/udacity/eda-course-materials/lesson6\")\n",
    "\n",
    "list.files()\n",
    "library(readxl)\n",
    "install.packages(\"tidyr\")\n",
    "library(dplyr)\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(tidyr)\n",
    "library(tibble)\n",
    "\n",
    "\n",
    "# Read and clean headers \n",
    "datafile <- read_excel(\"Indicator_Average age.xlsx\")\n",
    "\n",
    "library(readxl)\n",
    "gni <- read_excel(\"indicatorGNIpercapitaPPP.xlsx\")\n",
    "\n",
    "# rename columns datafile2\n",
    "colnames(datafile2)<-c(\"country\",\"2004\",\"2005\",\"2006\",\"2007\")\n",
    "\n",
    "# drop rows with only zero values\n",
    "datafile3 <-datafile2[rowSums(datafile2[,-1])>0,]\n",
    "\n",
    "# Create a new dataframe with \"country\",\"year\",\"value\" columns \n",
    "# use gather()\n",
    "da <- gather(datafile3, \"year\", \"value\", 2:5)\n",
    "\n",
    "#  use melt()\n",
    "b1m<-melt(datafile3,id=\"country\", variable = \"year\")\n",
    "\n",
    "# change the variable name value to av.age\n",
    "colnames(da) <- c(\"Country\",\"Year\",\"Av.age\")\n",
    "\n",
    "# Create a histogram\n",
    "qplot(x = Av.age, data = da, bins = 30)\n",
    "ggsave(\"histogram_da_billionaires.png\")\n",
    "\n",
    "p1 <- qplot(x = Av.age, data = da, bins = 30)\n",
    "p2 <- qplot(x = Av.age, data = da, bins = 10)\n",
    "p3 <- ggplot(da, aes(x= Av.age))+ geom_histogram(binwidth = 2)\n",
    "p4 <- ggplot(da, aes(x= Av.age))+ geom_dotplot(binwidth = 1)\n",
    "grid.arrange(p1, p2,p3,p4, ncol = 1)\n",
    "ggsave(\"Rplot2.png\")\n",
    "\n",
    "#Notes\n",
    "# We have outliers at 0 and 24 years, highest occurance of billionaires is \n",
    "# between 62 - 69 years.  The zero value is due to bad data and at 24 its probably inheritance.\n",
    "\n",
    "# using ggplot\n",
    "ggplot(da, aes(x= Av.age))+ geom_histogram(binwidth = 2)\n",
    "ggplot(da, aes(x= Av.age))+ geom_dotplot(binwidth = 1)\n",
    "\n",
    "# Try limiting the x-axis, altering the bin width, and setting different breaks on the x-axis.\n",
    "qplot(x = Av.age, data = da, bins = 30)\n",
    "  scale_x_continuous(limits = c(20, 80)) + breaks=seq(20,80,2)\n",
    "  \n",
    "ggplot(da, aes(x= Av.age))+ geom_histogram(binwidth = 2) +\n",
    "  scale_x_continuous(limits = c(20, 80),breaks=seq(20,80,4))\n",
    "ggsave('limitedhistogram.png')\n",
    "\n",
    "\n",
    "# control layout by adding options to the facet, ie nrow = 1 or ncol = 1\n",
    "ggplot(da, aes(x= Av.age))+ geom_histogram(binwidth = 2) +\n",
    "  scale_x_continuous(limits = c(20, 80),breaks=seq(20,80,4)) +\n",
    "  facet_wrap(~Year, ncol= 1)\n",
    "ggsave('av.agebyyearfacet.png')\n",
    "\n",
    "# get a better plot by letting the y axes vary freely i.e facet_wrap(~color, scales = \"free_y\")\n",
    "ggplot(da, aes(x= Av.age))+ geom_histogram(binwidth = 2) +\n",
    "  scale_x_continuous(limits = c(20, 80),breaks=seq(20,80,4)) +\n",
    "  facet_wrap(~Year, ncol=1, scales = \"free_y\")\n",
    "#note - Other scales options are \"free_x\" and \"free_xy\"\n",
    "\n",
    "# Boxplot to summarise the Av.age variable\n",
    "# Investigate the Av.age using box plots,\n",
    "# numerical summaries, and one of the following categorical\n",
    "# variables: cut, clarity, or color.\n",
    "# side by side box plots, grouped according to years\n",
    "# The median average age for billionaires is approximately the same over the years\n",
    "ggplot(da, aes(x= Year, y = Av.age)) + geom_boxplot()\n",
    "# or\n",
    "qplot(x = Year, y = Av.age, data = da, geom = \"boxplot\") + coord_cartesian(ylim = c(0,80))\n",
    "\n",
    "# Example question - what is the Av.age range for the middle 50%\n",
    "# of the billionaires in the year 2004?  50 - 68 ..visual\n",
    "by(da$Av.age,da$Year,summary)  # exact figures\n",
    "\n",
    "# The center is similar unlike the spread of the years\n",
    "ggplot(da, aes(x= Year, y = Av.age))+ geom_boxplot() + coord_flip()\n",
    "ggsave('av.age_co_boxplot.png')\n",
    "# the interquartile range\n",
    "\n",
    "# A single variable boxplot \n",
    "ggplot(da, aes(x= factor(0), y = Av.age))+ geom_boxplot() + xlab(\"\") +\n",
    "  scale_x_discrete(breaks = NULL) + coord_flip()\n",
    "ggsave('av.age_single_boxplot.png')\n",
    "\n",
    "# Quantitative summaries\n",
    "# The mean, median av. age of all billionaires\n",
    "with(da, mean(Av.age))\n",
    "with(da, median(Av.age))\n",
    "\n",
    "# or\n",
    "summary(da$Av.age)   # better all in one\n",
    "\n",
    "# how many people are billionaires before  50?\n",
    "dim(subset(da, Av.age <= 50))  #  50\n",
    "\n",
    "# how many people are billionaires after 70?\n",
    "dim(subset(da, Av.age >= 70))  #  38\n",
    "\n",
    "# Av.age by years - by facet wraping\n",
    "#which year has the highest, lowest av.age of billionaires\n",
    "#which year has the lowest median Av.age\n",
    "# highest 2005, lowest 2004, 2004\n",
    "by(da$Av.age,da$Year,summary, digits = max(getOption('digits')))\n",
    "#note - # the median and quartiles are reasonably close to each other which means that their\n",
    "# distributions ought to be similar - true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GoldbergData/Lesson-7-Explore-Many-Variables/blob/master/lesson5_student.md\n",
    "# Lesson 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-131ad0da2dd2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-131ad0da2dd2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ---\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "---\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---\n",
    "clive kmg\n",
    "18/10/2018\n",
    "\n",
    "Lesson 5(7)\n",
    "========================================================\n",
    "```{r}\n",
    "setwd(\"~/career_da-pffs/udacity-eda/eda-course-materials/lesson5\")\n",
    "```\n",
    "\n",
    "\n",
    "```{r Third Qualitative Variable}\n",
    "pf <- read.csv(\"pseudo_facebook.tsv\", sep = '\\t')\n",
    "```\n",
    "\n",
    "***\n",
    "```{r}\n",
    "\n",
    "library(readr)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(data.table)\n",
    "```\n",
    "\n",
    "### Multivariate Data\n",
    "Notes: This lesson goes beyond bivariate analysis.\n",
    "\n",
    "Moira Perceived Audience Size Colored by Age\n",
    "Notes: Adding color to represent age, but there was no pattern. Too much overplotting, which makes color as an encoder less effective.\n",
    "\n",
    "#Third Qualitative Variable\n",
    "Notes: Conducting EDA can lead to deadends. Don't panic.\n",
    "\n",
    "***\n",
    "\n",
    "### Moira Perceived Audience Size Colored by Age\n",
    "Notes:\n",
    "\n",
    "```{r Plotting Conditional Summaries}\n",
    "\n",
    "ggplot(aes(x = gender, y = age),\n",
    "       data = subset(pf, !is.na(gender))) + \n",
    "  geom_boxplot() +\n",
    "  stat_summary(geom = 'point', shape = 4, fun.y = mean)\n",
    "\n",
    "ggplot(aes(x = age, y = friend_count, color = gender), \n",
    "       data = subset(pf, !is.na(gender))) +\n",
    "  geom_line(aes(color = gender), stat = 'summary', fun.y = median)\n",
    "\n",
    "```\n",
    "```\n",
    "### Third Qualitative Variable\n",
    "Notes:\n",
    "# Write code to create a new data frame,\n",
    "# called 'pf.fc_by_age_gender', that contains\n",
    "# information on each age AND gender group.\n",
    "\n",
    "# The data frame should contain the following variables:\n",
    "\n",
    "#    mean_friend_count,\n",
    "#    median_friend_count,\n",
    "#    n (the number of users in each age and gender grouping)\n",
    "\n",
    "# Here is an example of the structure of your data frame. Your\n",
    "# data values will be different. Note that if you are grouping by\n",
    "# more than one variable, you will probably need to call the\n",
    "# ungroup() function. \n",
    "\n",
    "#   age gender mean_friend_count median_friend_count    n\n",
    "# 1  13 female          247.2953                 150  207\n",
    "# 2  13   male          184.2342                  61  265\n",
    "# 3  14 female          329.1938                 245  834\n",
    "# 4  14   male          157.1204                  88 1201\n",
    "\n",
    "# See the Instructor Note for two hints.\n",
    "\n",
    "You can include multiple variables to split the data frame when using group_by() function in the dplyr package.\n",
    "\n",
    "#new_groupings <- group_by(data, variable1, variable2)\n",
    "\n",
    "OR\n",
    "using chained commands...\n",
    "\n",
    "#new_data_frame <- data_frame %>%\n",
    "  group_by(variable1, variable2) %>%\n",
    "\n",
    "Repeated use of summarise() and group_by(): The summarize function will automatically remove one level of grouping (the last group it collapsed).\n",
    "\n",
    "http://stackoverflow.com/questions/21653295/dplyr-issues-with-group-by\n",
    "```{r}\n",
    "library(dplyr)\n",
    "#chain functions together %>%\n",
    "pf.fc_by_age_gender <- pf %>%\n",
    "  filter(!is.na(gender)) %>%  \n",
    "  group_by(age, gender)  %>%\n",
    "  summarise(mean_friend_count= mean(friend_count),\n",
    "            median_friend_count = median(friend_count),\n",
    "            n = n()) %>%\n",
    "  ungroup() %>%\n",
    "  arrange(age)\n",
    "  \n",
    "  #use the subset or filter command\n",
    "```\n",
    "\n",
    "#or\n",
    "\n",
    "age_gender_group <- group_by(pf, age, gender)\n",
    "age_gender_group <- filter(age_gender_group, !is.na(gender))\n",
    "pf.fc_by_age_gender <- summarise(age_gender_group,\n",
    "                                 mean_friend_count = mean(friend_count),\n",
    "                                 median_friend_count = median(friend_count),\n",
    "                                 n = n())\n",
    "arrange(pf.fc_by_age_gender, age)\n",
    "```\n",
    "```{r}\n",
    "head(pf.fc_by_age_gender, 3)\n",
    "```\n",
    "``\n",
    "***\n",
    "### Plotting Conditional Summaries\n",
    "Notes:\n",
    "# Create a line graph showing the\n",
    "# median friend count over the ages\n",
    "# for each gender. Be sure to use\n",
    "# the data frame you just created,\n",
    "# pf.fc_by_age_gender.\n",
    "Your code should look similar to the code we used to make the plot the first time. It will not need to make use of the stat and fun.y parameters.\n",
    "\n",
    "```{r Plotting Conditional Summaries}\n",
    "ggplot(aes(x = age, y = median_friend_count, color = gender), \n",
    "       data = pf.fc_by_age_gender) +\n",
    "  geom_line()\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Thinking in Ratios\n",
    "Notes: Looking at the graph especially in the young group how many more times is the average female user more than the average male user-- lets put it in relative terms.  Which leads us to the next topic!\n",
    "***\n",
    "\n",
    "### Wide and Long Format\n",
    "https://s3.amazonaws.com/udacity-hosted-downloads/ud651/DataWranglingWithR.pdf\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "### Reshaping Data\n",
    "https://seananderson.ca/2013/10/19/reshape/\n",
    "Notes:\n",
    "Can transform data via dplyr + tidyr or with reshape library.\n",
    "Reshaping Data\n",
    "```{r}\n",
    "install.packages(\"stringi\",type=\"win.binary\") \n",
    "library(reshape2)\n",
    "\n",
    "pf.fc_by_age_gender.wide <- dcast(pf.fc_by_age_gender,\n",
    "                                  age ~ gender,\n",
    "                                  value.var = 'mean_friend_count')\n",
    "head(pf.fc_by_age_gender.wide)\n",
    "```\n",
    "```{r}\n",
    "# same effect with dplyr and tidyr\n",
    "pf.fc_by_age_gender.wide2 <-\n",
    "  subset(pf.fc_by_age_gender[c('age', 'gender', 'median_friend_count')],\n",
    "         !is.na(gender)) %>%\n",
    "  spread(gender, median_friend_count) %>%\n",
    "  mutate(ratio = female / male)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# try out melt\n",
    "pf.fc_by_age_gender.long <- melt(pf.fc_by_age_gender.wide,\n",
    "                                 measure.vars = c('male', 'female'),\n",
    "                                 value.name = 'age')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Ratio Plot\n",
    "Notes:# Plot the ratio of the female to male median\n",
    "# friend counts using the data frame\n",
    "# pf.fc_by_age_gender.wide.\n",
    "\n",
    "# Think about what geom you should use.\n",
    "# Add a horizontal line to the plot with\n",
    "# a y intercept of 1, which will be the\n",
    "# base line. Look up the documentation\n",
    "# for geom_hline to do that. Use the parameter\n",
    "# linetype in geom_hline to make the\n",
    "# line dashed.\n",
    "\n",
    "# The linetype parameter can take the values 0-6:\n",
    "# 0 = blank, 1 = solid, 2 = dashed\n",
    "# 3 = dotted, 4 = dotdash, 5 = longdash\n",
    "# 6 = twodash\n",
    "\n",
    "```{r Ratio Plot}\n",
    "ggplot(aes(x = age, y = female / male),\n",
    "       data = pf.fc_by_age_gender.wide) + \n",
    "  geom_line() + \n",
    "  geom_hline(yintercept = 1, alpha = 0.3, linetype = 2)\n",
    "\n",
    "#or\n",
    "\n",
    "ggplot(aes(x = age, y = ratio), data = pf.fc_by_age_gender.wide2) +\n",
    "  geom_line() +\n",
    "  geom_hline(yintercept = 1, alpha = 0.3, linetype = 'dashed')\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Third Quantitative Variable\n",
    "A common mistake is to use tenure rather than pf$tenure or with(pf, tenure...). Remember that you need to access the variable in the data frame.\n",
    "Notes:# Create a variable called year_joined\n",
    "# in the pf data frame using the variable\n",
    "# tenure and 2014 as the reference year.\n",
    "\n",
    "# The variable year joined should contain the year\n",
    "# that a user joined facebook.\n",
    "\n",
    "#Hint 1:\n",
    "Divide the tenure variable by a number. Tenure is measured in days, but we want to convert it to years. \n",
    "#Hint 2: \n",
    "Subtract tenure measured in years from 2014. What does the decimal portion represent? Should we round up or round down to the closest year? \n",
    "#Hint 3: \n",
    "You can use the floor() function to round down to the nearest integer. You can use the ceiling() function to round up to the nearest integer. Which one should you use?\n",
    "\n",
    "```{r Third Quantitative Variable}\n",
    "pf$year_joined <- floor(2014 - pf$tenure/365)\n",
    "\n",
    "length(pf$year_joined)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Cut a Variable\n",
    "The cut function is useful for turning continuous variables into factors. eg\n",
    "> c1 <- cut(clinical.trial$age, breaks = 4)\n",
    "```{r}\n",
    "## specify break points explicitly using seq function\n",
    " \n",
    "## look what seq does  \n",
    "seq(30, 80, by = 10)   \n",
    "# ans= 30 40 50 60 70 80 \n",
    "## cut the age variable using the seq defined above\n",
    "c1 <- cut(clinical.trial$age, breaks = seq(30, 80, by = 10))\n",
    "## table of the resulting factor           \n",
    "table(c1)\n",
    "```\n",
    "\n",
    "Notes:\n",
    "# Create a new variable in the data frame\n",
    "# called year_joined.bucket by using\n",
    "# the cut function on the variable year_joined.\n",
    "\n",
    "# You need to create the following buckets for the\n",
    "# new variable, year_joined.bucket\n",
    "\n",
    "#        (2004, 2009]\n",
    "#        (2009, 2011]\n",
    "#        (2011, 2012]\n",
    "#        (2012, 2014]\n",
    "\n",
    "# Note that a parenthesis means exclude the year and a\n",
    "# bracket means include the year.\n",
    "\n",
    "# Look up the documentation for cut or try the link\n",
    "# in the Instructor Notes to accomplish this task.\n",
    "http://www.r-bloggers.com/r-function-of-the-day-cut-2/\n",
    "\n",
    "```{r}\n",
    "summary(pf$year_joined)\n",
    "```\n",
    "```{r}\n",
    "table(pf$year_joined)\n",
    "```\n",
    "= 2005  2006  2007  2008  2009  2010  2011  2012  2013  2014 \n",
    "    9    15   581  1507  4557  5448  9860 33366 43588    70\n",
    "\n",
    "***\n",
    "```{r}\n",
    "\n",
    "pf$year_joined.bucket <- cut(pf$year_joined, c(2004, 2009, 2011, 2012, 2014))\n",
    "```\n",
    "\n",
    "\n",
    "### Plotting it All Together\n",
    "Notes:\n",
    "\n",
    "```{r Plotting it All Together}\n",
    "table(pf$year_joined.bucket, useNA = 'ifany')\n",
    "```\n",
    "# = (2004,2009] (2009,2011] (2011,2012] (2012,2014]        <NA> \n",
    "       6669       15308       33366       43658           2 \n",
    "***\n",
    "```{r}\n",
    "ggplot(data = subset(pf, !is.na(year_joined.bucket)), \n",
    "aes(x= age, y=friend_count)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "  stat = 'summary', fun.y = median)\n",
    "```\n",
    "\n",
    "\n",
    "### Plot the Grand Mean\n",
    "Notes: Use the grand mean for high-level observation on data.\n",
    "\n",
    "# Write code to do the following:\n",
    "\n",
    "# (1) Add another geom_line to code below\n",
    "# to plot the grand mean of the friend count vs age.\n",
    "\n",
    "# (2) Exclude any users whose year_joined.bucket is NA.\n",
    "\n",
    "# (3) Use a different line type for the grand mean.\n",
    "\n",
    "# As a reminder, the parameter linetype can take the values 0-6:\n",
    "\n",
    "# 0 = blank, 1 = solid, 2 = dashed\n",
    "# 3 = dotted, 4 = dotdash, 5 = longdash\n",
    "# 6 = twodash\n",
    "\n",
    "```{r Plot the Grand Mean}\n",
    "ggplot(data = subset(pf, !is.na(year_joined.bucket)),\n",
    "aes(x= age, y=friend_count)) +\n",
    "  geom_line(aes(color = year_joined.bucket), \n",
    "  stat = 'summary', fun.y = mean) +\n",
    "  geom_line(stat = 'summary', fun.y = mean, linetype = 2)\n",
    "  ggsave('grand_mean.png')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "\n",
    "ggplot(aes(x = age, y = friend_count), \n",
    "       data = subset(pf, !is.na(pf$year_joined.bucket))) +\n",
    "  geom_line(aes(color = factor(year_joined.bucket)), stat = 'summary', fun.y = mean) +\n",
    "  geom_line(color = 'black', linetype = 'dashed', stat = 'summary', fun.y = mean)\n",
    "```\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### Friending Rate\n",
    "Notes:\n",
    "\n",
    "```{r Friending Rate}\n",
    "with(subset(pf, tenure >= 1), summary(friend_count / tenure))\n",
    "```\n",
    "   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
    "  0.0000   0.0775   0.2205   0.6096   0.5658 417.0000\n",
    "***\n",
    "\n",
    "### Friendships Initiated\n",
    "Notes:\n",
    "\n",
    "What is the median friend rate? 0.221\n",
    "\n",
    "What is the maximum friend rate? 417.0\n",
    "\n",
    "```{r Friendships Initiated\n",
    "ggplot(aes(x = tenure, y = friendships_initiated /tenure),\n",
    "       data = subset(pf, tenure >= 1)) +\n",
    "    geom_line(aes(color = year_joined.bucket),)\n",
    "    ggsave('friendships_initiated.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "```{r}\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Bias-Variance Tradeoff Revisited\n",
    "Understanding the Bias-Variance Tradeoff\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "NOTE: The code changing the binning is substituting x = tenure in the plotting expressions with x = 7 * round(tenure / 7), etc., binning values by the denominator in the round function and then transforming back to the natural scale with the constant in front.\n",
    "\n",
    "\n",
    "Notes: To get rid of the noise you have to vary the bin size differently eg from change tenure from 7, 30 to 90 and see the difference \n",
    "\n",
    "```{r Bias-Variance Tradeoff Revisited}\n",
    "library(gridExtra)\n",
    "p1 <- ggplot(aes(x = tenure, y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure >= 1)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "            stat = 'summary',\n",
    "            fun.y = mean)\n",
    "\n",
    "p2 <- ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure > 0)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "            stat = \"summary\",\n",
    "            fun.y = mean)\n",
    "\n",
    "p3 <- ggplot(aes(x = 30 * round(tenure / 30), y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure > 0)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "            stat = \"summary\",\n",
    "            fun.y = mean)\n",
    "\n",
    "p4 <- ggplot(aes(x = 90 * round(tenure / 90), y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure > 0)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "            stat = \"summary\",\n",
    "            fun.y = mean)\n",
    "grid.arrange(p1, p2, p3, p4, ncol=1)\n",
    "ggsave('biastradeoff.png')\n",
    "\n",
    "```\n",
    "```{r}\n",
    "ggplot(aes(x = 90 * round(tenure / 90), y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure > 0)) +\n",
    "  geom_line(aes(color = year_joined.bucket),\n",
    "            stat = \"summary\",\n",
    "            fun.y = mean)\n",
    "```\n",
    "\n",
    "# Instead of geom_line(), use geom_smooth() to add a smoother to the plot.\n",
    "# You can use the defaults for geom_smooth() but do color the line\n",
    "# by year_joined.bucket\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(x = tenure, y = friendships_initiated / tenure),\n",
    "       data = subset(pf, tenure > 0)) +\n",
    "  geom_smooth(aes(color = year_joined.bucket))\n",
    "```\n",
    "\n",
    "Friendships initiated decreases as tenure increases!\n",
    "***\n",
    "\n",
    "### Sean's NFL Fan Sentiment Study\n",
    "Notes:Notes: It took averaging to get the story to come out for fan sentiment related to NFL team wins and losses. If data is too noisy, average until you see a reasonable story or a story you'd expect. As he added more data, he had lower variance but more biased data.\n",
    "interesting!!!\n",
    "***\n",
    "\n",
    "### Introducing the Yogurt Data Set\n",
    "Notes:\n",
    "#read in the data\n",
    "\n",
    "```{r}\n",
    "rm('yogurt')\n",
    "yo <- read.csv('yogurt.csv')\n",
    "\n",
    "str(yo)\n",
    "\n",
    "#change the id from an int to a factor\n",
    "yo$id <- factor(yo$id)\n",
    "str(yo)\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Histograms Revisited\n",
    "Notes:If the binwidth is increased, the discreteness of the data disappears, creating a biased model. You can average too much, which would lead to biasness depending on the underlying characteristics of the data set\n",
    "left long tailed data with bw = 10\n",
    "\n",
    "\n",
    "```{r Histograms Revisited}\n",
    "ggplot(aes(x= price), data = yo) +\n",
    "  geom_histogram(binwidth = 1)  \n",
    "\n",
    "```\n",
    "Notice - there is discretness in the plot.  some prices have many observations while adjacent ones have nothing\n",
    "-with big binwidth like 10 we have no discretness - details are obscurred\n",
    "There is price variation for the products.\n",
    "For this data, this histogram is a very biased model since change of binwidth obscures the picture of the data.  Are there other ways we could have noticed this issue....?\n",
    "Like other exploratory methods\n",
    "***\n",
    "```{r}\n",
    "#shows distinct prices/types\n",
    "unique(yo$price)\n",
    "length(unique(yo$price))\n",
    "table(yo$price)\n",
    "#gives you a picture simr to the histogram\n",
    "```\n",
    "\n",
    "\n",
    "### Number of Purchases\n",
    "Notes:# Create a new variable called all.purchases,\n",
    "# which gives the total counts of yogurt for\n",
    "# each observation or household.\n",
    "\n",
    "# One way to do this is using the transform\n",
    "# function. You can look up the function transform\n",
    "# and run the examples of code at the bottom of the\n",
    "# documentation to figure out what it does.\n",
    "\n",
    "# The transform function produces a data frame\n",
    "# so if you use it then save the result to 'yo'!\n",
    "\n",
    "# OR you can figure out another way to create the\n",
    "# variable.\n",
    "\n",
    "?transform()\n",
    "\n",
    "```{r Number of Purchases}\n",
    "yo <- transform(yo, all.purchases = strawberry + blueberry + pina.colada + plain + mixed.berry)\n",
    "\n",
    "summary(yo$all.purchases)\n",
    "\n",
    "qplot(x = all.purchases, data = yo, binwidth = 1,\n",
    "      fill = I('#099DD9'))\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Prices over Time\n",
    "Notes:\n",
    "# Create a scatterplot of price vs time.\n",
    "\n",
    "# This will be an example of a time series plot.\n",
    "\n",
    "# Resolve overplotting issues by using\n",
    "# techniques you learned in Lesson 4.\n",
    "\n",
    "# What are some things that you notice?\n",
    "\n",
    "```{r Prices over Time}\n",
    "ggplot(aes(x = time, y= price), data = yo) +\n",
    "  geom_jitter(alpha = 1/5, shape = 21)\n",
    "  \n",
    "  #or\n",
    "ggplot(data = yo, aes(x = time, y = price)) + \n",
    "  geom_jitter(alpha = 1/4, shape = 21, fill = I('#F79420'))\n",
    "ggsave('price_over_time.png')\n",
    "```\n",
    "Prices were generally higher at the end of the time period. There was some lumpiness in the data, which could be indicative of possible coupon usage, or experimentation in differently priced yogurts. A few outliers had lower prices towards the end of the time period.\n",
    "lower prices may be experiments till the right higher price is set\n",
    "***\n",
    "\n",
    "### My Sampling Observations\n",
    "Notes: best way to get a good fill with a dataset\n",
    "Consumer behavior is quiet varied\n",
    "some buy  alot with different price ranges\n",
    "some change from a low to a higher price type and then keep steady\n",
    "while others do the opposite - ie move  from a high price type to a lower price type and stay there.  some are low and do not change and  are steady overall\n",
    "***\n",
    "\n",
    "### Looking at Samples of Households\n",
    "Note: x %in% y returns a logical (boolean) vector the same length as x that says whether each entry in x appears in y. That is, for each entry in x, it checks to see whether it is in y. This allows us to subset the data so we get all the purchases occasions for the households in the sample. Then, we create scatterplots of price vs. time and facet by the sample id.\n",
    "\n",
    "Use the pch or shape parameter to specify the symbol when plotting points. Scroll down to 'Plotting Points' on QuickR's Graphical Parameters\n",
    "https://www.statmethods.net/advgraphs/parameters.html\n",
    "\n",
    "# id %in%   means loop over the ids and go create a panel plot\n",
    "\n",
    "```{r Looking at Sample of Households}\n",
    "set.seed(1026)\n",
    "sample_id <- unique(yo$id)\n",
    "sample.ids <- sample(x = sample_id, size = 16)\n",
    "\n",
    "ggplot(aes(x = time, y = price),\n",
    "       data = subset(yo, id %in% sample.ids)) +\n",
    "  facet_wrap( ~ id) +\n",
    "  geom_line() +\n",
    "  geom_point(aes(size = all.purchases), pch = 1)\n",
    "  ggsave('panelplot_households.png')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### The Limits of Cross Sectional Data\n",
    "Notes:The yogurt data set unlike the pf data allows us to facet households over a time series.  pf data is cross sectional only, meaning its a snap shot at  a fixed given point therefore can not be explored in this way.\n",
    "\n",
    "***\n",
    "\n",
    "### Many Variables\n",
    "Notes: Plotting many variables at once can provide insight into potentially impactful variables that may not have been up interest before.\n",
    "\n",
    "\n",
    "Here's the scatterplot matrix as a pdf.\n",
    "https://s3.amazonaws.com/udacity-hosted-downloads/ud651/scatterplotMatrix.pdf\n",
    "\n",
    "You'll need to run the code install.packages('GGally') to install the package for creating this particular scatterplot matrix.\n",
    "\n",
    "If the plot takes a long time to render or if you want to see some of the scatterplot matrix, then only examine a smaller number of variables. You can use the following code or select fewer variables. We recommend including gender (the 6th variable)!\n",
    "\n",
    "pf_subset = pf[, c('age', 'dob_year', 'dob_month', 'gender', 'tenure')]\n",
    "\n",
    "You can also select a subset using the subset() function and the \"select\" argument:\n",
    "\n",
    "pf_subset <- subset(pf, select = -c(userid, year_joined, year_joined_bucket))\n",
    "\n",
    "The - sign in the \"select\" value indicates all but the listed columns.\n",
    "\n",
    "You may find in your matrix that variable labels are on the outer edges of the scatterplot matrix, rather than on the diagonal. If you want labels in the diagonal, you can set the axisLabels = 'internal' argument in your ggpairs command.\n",
    "\n",
    "Have questions? Head to the forums for discussion with the Udacity Community.\n",
    "\n",
    "\n",
    "***\n",
    "```{r}\n",
    "install.packages('GGally')\n",
    "library('GGally')\n",
    "theme_set(theme_minimal(20))\n",
    "\n",
    "# set seed for reproducible results\n",
    "set.seed(1836)\n",
    "pf_subset <- pf[, c(2:15)]\n",
    "names(pf_subset)\n",
    "```\n",
    "\n",
    "#or http://www.sthda.com/english/wiki/ggally-r-package-extension-to-ggplot2-for-correlation-matrix-and-survival-plots-r-software-and-data-visualization\n",
    "\n",
    "Loading GGally package\n",
    "\n",
    "library(\"GGally\")\n",
    "\n",
    "ggcorr(): Plot a correlation matrix\n",
    "\n",
    "The function ggcorr() draws a correlation matrix plot using ggplot2.\n",
    "\n",
    "The simplified format is:\n",
    "\n",
    "ggcorr(data, palette = \"RdYlGn\", name = \"rho\", \n",
    "       label = FALSE, label_color = \"black\",  ...)\n",
    "\n",
    "\n",
    "    data: a numerical (continuous) data matrix\n",
    "    palette: a ColorBrewer palette to be used for correlation coefficients. Default value is “RdYlGn”.\n",
    "    name: a character string used for legend title.\n",
    "    label: logical value. If TRUE, the correlation coefficients are displayed on the plot.\n",
    "    label_color: color to be used for the correlation coefficient\n",
    "\n",
    "\n",
    "The function ggcorr() can be used as follow:\n",
    "\n",
    "# Prepare some data\n",
    "df <- mtcars[, c(1,3,4,5,6,7)]\n",
    "# Correlation plot\n",
    "ggcorr(df, palette = \"RdBu\", label = TRUE)\n",
    "\n",
    "\n",
    "### Scatterplot Matrix\n",
    "Notes:\n",
    "ggpairs(pf_subset[sample.int(nrow(pf_subset), 1000), ])\n",
    "\n",
    "```{r}\n",
    "ggpairs(pf_subset[sample.int(nrow(pf_subset), 1000), ], axisLabels = 'internal')\n",
    "ggsave('scatterplotmatrix.png')\n",
    "\n",
    "ggcorr(pf, palette = \"RdBu\", label = TRUE)\n",
    "ggsave('scattermatrix2.png')\n",
    "\n",
    "ggpairs(pf)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "ggpairs(pf_subset[sample.int(nrow(pf_subset), 1000), ], axisLabels = 'internal')\n",
    "\n",
    "***\n",
    "\n",
    "### Even More Variables\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "### Heat Maps\n",
    "Notes:\n",
    "\n",
    "```{r}\n",
    "nci <- read.table(\"nci.tsv\")\n",
    "colnames(nci) <- c(1:64)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "nci.long.samp <- melt(as.matrix(nci[1:200,]))\n",
    "names(nci.long.samp) <- c(\"gene\", \"case\", \"value\")\n",
    "head(nci.long.samp)\n",
    "\n",
    "ggplot(aes(y = gene, x = case, fill = value),\n",
    "  data = nci.long.samp) +\n",
    "  \n",
    "  geom_tile() +\n",
    "  scale_fill_gradientn(colours = colorRampPalette(c(\"blue\", \"red\"))(100))\n",
    "```\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### Analyzing Three of More Variables\n",
    "Reflection:\n",
    "Basic techniques from previous lessons were extended further to look at patterns across many variables at once.  Extensions were added\n",
    " to the scatterplot.  summations were added for multiple groups and we saw techniques to deal with large data at once to come up with \n",
    "scatterplot matrix and heatmap.  we also learn how to reshape data from a Broad data with one row/case to aggregate data with one row\n",
    "    per commbination of variables.  And we moved back and forth from long data to wide data.\n",
    " https://rstudio-pubs-static.s3.amazonaws.com/284678_dd65fd6fd3e54cb186ff3658f706b2e5.html   \n",
    "***\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/284678_dd65fd6fd3e54cb186ff3658f706b2e5.html\n",
    "    \n",
    "Click **KnitHTML** to see all of your hard work and to have an html\n",
    "page of this lesson, your answers, and your notes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3638777bd2c8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-3638777bd2c8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    miscellaneous - during gapminder challenges\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "miscellaneous - during gapminder challenges\n",
    "\n",
    "https://towardsdatascience.com/exploratory-data-analysis-in-r-of-global-data-from-gapminder-13c5de5bee70\n",
    "\n",
    "# some data wrangling needed to ensure I can use the data in the type I need later on\n",
    "hiv.T <- gather(data=hiv, key='Year', value='HIV_prev', '1979':'2011', convert = TRUE)\n",
    "hiv.T$HIV_prev <- as.numeric(hiv.T$HIV_prev)\n",
    "\n",
    "\n",
    "# combining dataframes into one for analysis\n",
    "gdp.HIV <- merge(gdp.T, hiv.T)\n",
    "str(gdp.HIV)\n",
    "\n",
    "#look at the % of missing data\n",
    "(is.na(gdp.HIV$GDP)) / nrow(gdp.HIV) * 100\n",
    "# sometimes Rather than replacing the missing data with an average or an estimate, missing\n",
    "data will be dismissed (i.e. not plotted) in the subsequent analysis.\n",
    "\n",
    "#Let’s get an initial sense of the type of distribution\n",
    "(gdp.HIV$GDP)\n",
    "\n",
    "****relationship between population and life expectancy.\n",
    "library(tidyverse)\n",
    "library(gapminder)\n",
    "\n",
    "gapminder %>%\n",
    "  # convert to population in millions\n",
    "  mutate(pop = pop / 1000000) %>%\n",
    "  ggplot(aes(pop, lifeExp)) +\n",
    "  geom_point(alpha = .2) +\n",
    "  geom_smooth() +\n",
    "  scale_x_log10() +\n",
    "  labs(title = \"As population increases, life expectancy increases\",\n",
    "       x = \"Population (in millions)\",\n",
    "       y = \"Average life expectancy\")\n",
    "    \n",
    "    \n",
    "head(countries)[1:5]\n",
    "\n",
    "##Changing col names\n",
    "colnames(countries)[1] <- c(\"Country\")\n",
    "colnames(countries)[4] <- c(\"Code\")\n",
    "colnames(countries)[6] <- c(\"Region\")\n",
    "colnames(countries)[7] <- c(\"Sub.Region\")\n",
    "countries <- transmute(countries, Country, Code, Region, Sub.Region)\n",
    "head(countries)\n",
    "\n",
    "# Reshapping datasets\n",
    "# load reshape2 package\n",
    "library(reshape2)\n",
    "# reshape gdp_per_capita data frame\n",
    "\n",
    "gdp_per_capita <- melt(gdp_per_capita, id.vars=\"Country\", variable.name = \"Years\", value.name=\"GDP per capita\", na.rm = TRUE)\n",
    "\n",
    "\n",
    "# change factor to numeric\n",
    "gdp_per_capita$Years <- as.numeric(as.character(gdp_per_capita$Years))\n",
    "str(gdp_per_capita)\n",
    "\n",
    "\n",
    "# change factor to numeric\n",
    "population$Years <- as.numeric(as.character(population$Years))\n",
    "# population value has comma separators which need to be removed\n",
    "population$Population <- as.numeric(gsub(',', '', population$Population))\n",
    "str(population)\n",
    "\n",
    "Combining Datasets\n",
    "# Get an idea if it will be correct join\n",
    "head(left_join(gdp_per_capita, child_mortality, by=c(\"Country\", \"Years\")))\n",
    "\n",
    "# perform the left join\n",
    "gapdata <- left_join(gdp_per_capita, child_mortality, by=c(\"Country\",\"Years\"))\n",
    "head(gapdata)\n",
    "\n",
    "# Join the democracy_score data frame\n",
    "gapdata <- left_join(gapdata, democracy_score, by=c(\"Country\", \"Years\"))\n",
    "# Join the life_expectancy data frame\n",
    "gapdata <- left_join(gapdata, life_expectancy, by=c(\"Country\", \"Years\"))\n",
    "# Join the population data frame\n",
    "gapdata <- left_join(gapdata, population, by=c(\"Country\", \"Years\"))\n",
    "str(gapdata)\n",
    "\n",
    "# join the gapdata and countries data frame\n",
    "# see if we are doing the right thing\n",
    "head(left_join(gapdata, countries, by=\"Country\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 Diamonds and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lesson 6\n",
    "========================================================\n",
    "https://rpubs.com/profversaggi/lesson_6_diamonds\n",
    "    \n",
    "    \n",
    "### Welcome\n",
    "Notes:\n",
    "```{r}\n",
    "library(ggplot2)\n",
    "\n",
    "data(\"diamonds\")\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Scatterplot Review\n",
    "# Let's start by examining two variables in the data set.\n",
    "# The scatterplot is a powerful tool to help you understand\n",
    "# the relationship between two continuous variables.\n",
    "\n",
    "# We can quickly see if the relationship is linear or not.\n",
    "# In this case, we can use a variety of diamond\n",
    "# characteristics to help us figure out whether\n",
    "# the price advertised for any given diamond is \n",
    "# reasonable or a rip-off.\n",
    "\n",
    "# Let's consider the price of a diamond and it's carat weight.\n",
    "# Create a scatterplot of price (y) vs carat weight (x).\n",
    "\n",
    "# Limit the x-axis and y-axis to omit the top 1% of values\n",
    "\n",
    "summary(diamonds)\n",
    "\n",
    "```{r Scatterplot Review}\n",
    "ggplot(aes(x = carat, y = price), data = diamonds) + \n",
    "  xlim(0, quantile(diamonds$carat, 0.99)) +\n",
    "  ylim(0, quantile(diamonds$price, 0.99)) +\n",
    "    geom_point(fill = I('#F79420'), color = I('black'), shape = 21)\n",
    "\n",
    "or\n",
    "\n",
    "ggplot(aes(x = carat, y = price), data = diamonds) + \n",
    "  scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +\n",
    "  scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +\n",
    "    geom_point(fill = I('#F79420'), color = I('black'), shape = 21)\n",
    "  \n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "ggplot(aes(x=carat, y=price), data=diamonds) +\n",
    "  geom_point(fill=I(\"#F79420\"), color=I(\"black\"), shape=21) +\n",
    "  stat_smooth(method=\"lm\") +\n",
    "  scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99)) ) +\n",
    "  scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99)) ) +\n",
    "  ggtitle(\"Diamonds: Price vs. Carat\")\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Price and Carat Relationship\n",
    "Response:Price and Carat Relationship = What do you notice about the relationship here between price and carat?\n",
    "\n",
    "There is an obvious positive (non-linear) relationship between both variables: as carat size increases, price also increases. It could be exponential. Also, the variance of the relationship increases as carat size increases. There is also very clear discrete values that carat size takes on, which are those vertical strips on the graph.\n",
    "\n",
    "We can see that the linear trend line doesn’t go through the center of the data at some key places. It should curve in certain parts of the graph. It should slope up more towards the end. If we tried to use this for predictions, we might be off some key places inside and outside of the existing data that we have displayed.\n",
    "\n",
    "***\n",
    "\n",
    "### Frances Gerety\n",
    "Notes: interesting!\n",
    "\n",
    "#### A diamonds is   FOREVER !!!\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### The Rise of Diamonds\n",
    "Notes:\n",
    "\n",
    "***\n",
    "\n",
    "### ggpairs Function\n",
    "Notes:\n",
    "\n",
    "```{r ggpairs Function}\n",
    "# install these if necessary\n",
    "install.packages('GGally')\n",
    "install.packages('scales')\n",
    "install.packages('memisc')\n",
    "install.packages('lattice')\n",
    "install.packages('MASS')\n",
    "install.packages('car')\n",
    "install.packages('reshape')\n",
    "install.packages('plyr')\n",
    "\n",
    "# load the ggplot graphics package and the others\n",
    "library(ggplot2)\n",
    "library(GGally)\n",
    "\n",
    "# sample 10,000 diamonds from the data set\n",
    "\n",
    "```\n",
    "library(ggplot2)  - failed!!\n",
    "\n",
    "\n",
    "```{r}\n",
    "# sample 10,000 diamonds from the data set to get a snapshop of the large dataset\n",
    "\n",
    "set.seed(20022012)\n",
    "\n",
    "diamond_samp <- diamonds[sample(1:length(diamonds$price), 10000), ]\n",
    "\n",
    "ggpairs(diamond_samp, \n",
    "        lower = list(continuous = wrap(\"points\", shape = I('.'))), \n",
    "        upper = list(combo = wrap(\"box\", outlier.shape = I('.'))))\n",
    "\n",
    "#https://s3.amazonaws.com/udacity-hosted-downloads/ud651/ggpairs_landscape.pdf\n",
    "```\n",
    "\n",
    "Method #2: We can also use the psych package and create a scatterplot matrix with the pairs.panels() function. However, it does not create a mixture of scatterplots, boxplots, and histograms based on whether the variable is quantitative or qualitative. This is where ggpairs has an advantage over this function. However, pairs.panels() does not take as much time to plot as the method from above, which is a plus.\n",
    "```{r}\n",
    "#Alternative Scatterplot Matrix Function\n",
    "install.packages(\"psych\")\n",
    "library(psych)\n",
    "pairs.panels(diamond_samp,pch=\".\")\n",
    "```\n",
    "\n",
    "\n",
    "What are some things you notice in the ggpairs output?\n",
    "Response:\n",
    "\n",
    "    ** What’s happening?**\n",
    "\n",
    "    GGPairs is plotting each variable against the other in a pretty smart way.\n",
    "\n",
    "    In the lower triangle of the plot matrix, ggplot uses grouped histograms for qualitative - qualitative comparisons, and scatter plots for quantitative - quantitative pairs.\n",
    "\n",
    "    In the upper triangle, it plots group histograms for qualitative - qualitative pairs, this time using the X instead of the Y variable as the grouping factor, and box plots for qualitative - quantitative pairs.\n",
    "\n",
    "    It provides teh correlation for quantitative - quantitative pairs.\n",
    "\n",
    "    REMEMBER - our goal is to understand the price of diamonds, that’s the focus. Lets look at the relationships that correspond to price.\n",
    "\n",
    "What are some things you notice in the ggpairs output? Response:\n",
    "\n",
    " It is a positive, non linear relationship\n",
    " Price and carat are highly correlated - almost a 1 to 1.\n",
    "There “seems”\" to be relationships between price and clarity and price and color, which might come in handy as we model our data.\n",
    "\n",
    " The critical factor driving price is the carat weight of the diamond. The relationship between price and diamond size is non-linear. What might explain this pattern?\n",
    " On the supply side, larger contiguous diamonds with out flaws are harder to find than smaller ones, which might help explain the exponential curve.\n",
    " This is related to the fact that the weight of a diamond is a function of volume, while volume is a function of the length X width X height.\n",
    " This suggests that we might be especially interested in the cube’d root of carat weight.\n",
    "\n",
    "    NOTE - it’s often the case that leveraging substantive knowledge about your data like this can lead to especially fruitful transformatio\n",
    "\n",
    "***\n",
    "\n",
    "### The Demand of Diamonds\n",
    "On the demand side, customers in the market for a less expensive, smaller diamond are probably more sensitive to price than more well-to-do buyers. Many less than one carat customers would surely never buy a dimond were it not for the social norm of presenting one when proposing.\n",
    "\n",
    "Also, there are fewer customers who can afford a bigger diamond (one larger than one carat). Hence, we shouldn’t expect the market for bigger diamonds to be as competitive as the one for smaller diamonds. So it makes sense that the variance, as well as the price, would increase with carat size.\n",
    "\n",
    "Now, often the distribution of any monetary variable like dollars will be highly skewed and vary over orders of magnitude. This can result from path dependence, for example the rich getting richer or multiplicative processes like year on year inflation or some combination of both. Hence, it is a good idea to look into compressing any such variable by putting it on a log scale.\n",
    "\n",
    "Refer to this link for more information on when to tranform a variable: https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/.\n",
    "Notes:# Create two histograms of the price variable\n",
    "# and place them side by side on one output image.\n",
    "\n",
    "# We’ve put some code below to get you started.\n",
    "\n",
    "# The first plot should be a histogram of price\n",
    "# and the second plot should transform\n",
    "# the price variable using log10.\n",
    "\n",
    "# Set appropriate bin widths for each plot.\n",
    "# ggtitle() will add a title to each histogram.\n",
    "\n",
    "# You can self-assess your work with the plots\n",
    "# in the solution video.\n",
    "\n",
    "# ALTER THE CODE BELOW THIS LINE\n",
    "# ==============================================\n",
    "\n",
    "library(gridExtra)\n",
    "\n",
    "plot1 <- qplot() + \n",
    "  ggtitle('Price')\n",
    "\n",
    "plot2 <- qplot() +\n",
    "  ggtitle('Price (log10)')\n",
    "\n",
    "grid.arrange()\n",
    "\n",
    "```{r The Demand of Diamonds}\n",
    "library(gridExtra)\n",
    "\n",
    "plot1 <- ggplot(aes(price), data=diamonds) + \n",
    "  geom_histogram(binwidth = 150) +\n",
    "  ggtitle('Price')\n",
    "\n",
    "plot2 <- ggplot(aes(price), data=diamonds) +\n",
    "  geom_histogram(bibwidth = 0.15) +\n",
    "  ggtitle('Price (log10)') +\n",
    "  scale_x_log10()\n",
    "\n",
    "grid.arrange(plot1, plot2, ncol=2)\n",
    "\n",
    "# fill = I('#F79420')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Connecting Demand and Price Distributions\n",
    "Notes: When looking at these plots, what do you notice? Think specifically about the two peaks in the transformed plot and how it relates to the demand for diamonds.\n",
    "\n",
    "Response: We can see that the price for diamonds are heavily skewed (long right tail). However, when you look at price on the log10 scale, it seems much better behaved. It is a lot closer to the bell curve normal distribution. It even shows evidence of bimodality on this scale. There appear to be two different “populations.” One peak might represent the mean for the people that buy diamonds of size one carat or less. And the other mean represents the rich people that can afford higher priced diamonds.\n",
    "\n",
    "***\n",
    "\n",
    "### Scatterplot Transformation\n",
    "\n",
    "```{r Scatterplot Transformation}\n",
    "p1 <- ggplot(aes(carat, price), data=diamonds) +\n",
    "  geom_point() +\n",
    "  ggtitle(\"Price by Carat\")\n",
    "p2 <- ggplot(aes(carat, price), data=diamonds) +\n",
    "  geom_point() +\n",
    "  scale_y_continuous(trans=log10_trans()) +\n",
    "  ggtitle(\"Price (log10) by Carat\")\n",
    "grid.arrange(p1, p2, ncol=2)\n",
    "```\n",
    "\n",
    "\n",
    "### Create a new function to transform the carat variable\n",
    "On the log scale, the prices look less dispersed at the high end of carat size and price, but actually, we can do better. Let’s try using the cube root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. Remember, volume is on a cubic scale!\n",
    "\n",
    "```{r cuberoot transformation}\n",
    "cuberoot_trans = function() trans_new('cuberoot', transform = function(x) x^(1/3),\n",
    "                                      inverse = function(x) x^3)\n",
    "\n",
    "```\n",
    "\n",
    "Then, when we get to our actual ggplot command, what we’ll do is we’ll use the scale_x_continuous argument and transform the x-axis with this cube root transformation function. We are also transforming the y-axis with the log10 transformation.\n",
    "\n",
    "#### Use the cuberoot_trans function\n",
    "```{r Use cuberoot_trans}\n",
    "ggplot(aes(carat, price), data = diamonds) + \n",
    "  geom_point() + \n",
    "  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),\n",
    "                     breaks = c(0.2, 0.5, 1, 2, 3)) + \n",
    "  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),\n",
    "                     breaks = c(350, 1000, 5000, 10000, 15000)) +\n",
    "  ggtitle('Price (log10) by Cube-Root of Carat')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Overplotting Revisited - multiple plots taken on the same values ...due to rounding off.\n",
    "We still have to deal with overplotting, which occurs when multiple points take on the same value. Oftentimes, this occurs because of rounding. The following code allows us to see the carat and price values that occur the most frequently. These repeated values can really obscure the density and the sparsity of our data and really key points .  To show how stacked they are use:\n",
    "#.\n",
    "solved by jittering, making points smaller and adding transparency.\n",
    "To work around overplotting, the alpha, size, and and jitter option can be used for our plot.\n",
    "\n",
    "```{r Sort and Head Tables}\n",
    " head(sort(table(diamonds$carat), decreasing=T))\n",
    "```\n",
    "\n",
    "\n",
    "```{r Overplotting Revisited}\n",
    "\n",
    "ggplot(aes(carat, price), data = diamonds) + \n",
    "  geom_point(alpha=1/10, size=3/4, position = \"jitter\") + \n",
    "  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),\n",
    "                     breaks = c(0.2, 0.5, 1, 2, 3)) + \n",
    "  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),\n",
    "                     breaks = c(350, 1000, 5000, 10000, 15000)) +\n",
    "  ggtitle('Price (log10) by Cube-Root of Carat')\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Other Qualitative Factors\n",
    "Notes: We can see what looks like an almost linear relationship between carat weight and price after the transformations. However, there are other factors that influence the price of a diamond. Clarity also factors into price. However, people commonly search for a diamond of a specific size, so we shouldn’t expect clarity to be as strong a factor as size (carat). Cut can also have a major impact on the fiery trait that diamonds have, so we would expect it to have some effect on price.\n",
    "\n",
    "***\n",
    "\n",
    "### Price vs. Carat and Clarity\n",
    "# Adjust the code below to color the points by clarity.\n",
    "\n",
    "# A layer called scale_color_brewer() has \n",
    "# been added to adjust the legend and\n",
    "# provide custom colors.\n",
    "\n",
    "# See if you can figure out what it does.\n",
    "# Links to resources are in the Instructor Notes.\n",
    "\n",
    "# You will need to install the package RColorBrewer\n",
    "# in R to get the same colors and color palettes.\n",
    "https://ggplot2.tidyverse.org/reference/scale_brewer.html\n",
    "\n",
    "Alter the code below.\n",
    "```{r Price vs. Carat and Clarity}\n",
    "# install and load the RColorBrewer package\n",
    "install.packages('RColorBrewer')\n",
    "library(RColorBrewer)\n",
    "\n",
    "ggplot(aes(x = carat, y = price, color=clarity), data = diamonds) + \n",
    "  geom_point(alpha = 0.5, size = 1, position = 'jitter') +\n",
    "  scale_color_brewer(type = 'div',\n",
    "    guide = guide_legend(title = 'Clarity', reverse = T,\n",
    "    override.aes = list(alpha = 1, size = 2))) +  \n",
    "  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),\n",
    "    breaks = c(0.2, 0.5, 1, 2, 3)) + \n",
    "  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),\n",
    "    breaks = c(350, 1000, 5000, 10000, 15000)) +\n",
    "  ggtitle('Price (log10) by Cube-Root of Carat and Clarity')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Clarity and Price\n",
    "Response: Clarity does explain some of the change in price because it is clear that diamonds that are “IF” are the most expensive whereas “I1” are the least expensive clarity types. In other words, holding carat weight constant, we see that diamonds with lower clarity are almost always cheaper than diamonds with better clarity. Clarity explains a lot of the variance found in price!\n",
    "\n",
    "***\n",
    "\n",
    "### Price vs. Carat and Cut\n",
    "\n",
    "Alter the code below.\n",
    "```{r Price vs. Carat and Cut}\n",
    "ggplot(aes(x = carat, y = price, color = cut), data = diamonds) + \n",
    "  geom_point(alpha = 0.5, size = 1, position = 'jitter') +\n",
    "  scale_color_brewer(type = 'div',\n",
    "                     guide = guide_legend(title = 'Cut', reverse = T,\n",
    "                                          override.aes = list(alpha = 1, size = 2))) +  \n",
    "  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),\n",
    "                     breaks = c(0.2, 0.5, 1, 2, 3)) + \n",
    "  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),\n",
    "                     breaks = c(350, 1000, 5000, 10000, 15000)) +\n",
    "  ggtitle('Price (log10) by Cube-Root of Carat and Cut')\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Cut and Price\n",
    "Response:I believe that cut does NOT account for a huge chunk of the variance in price like clarity did. However, it does appear to account for a tiny percentage of the price variance. Keeping carat constant, we see some of the browns (fair and good) at the bottom, some of the white (very good) and light green (premium) at the middle, and the dark green (ideal) at the top. However, the division between these categories is not as clear as it was for clarity.\n",
    "\n",
    "Solomon believes the answer to this question is debatable. He says that we don’t see much variation on cut. Most of the diamonds in the data are ideal cut anyway, so we lost the pattern that we saw before for clarity. ***\n",
    "\n",
    "***\n",
    "\n",
    "### Price vs. Carat and Color\n",
    "\n",
    "Alter the code below.\n",
    "```{r Price vs. Carat and Color}\n",
    "ggplot(aes(x = carat, y = price, color = color), data = diamonds) + \n",
    "  geom_point(alpha = 0.5, size = 1, position = 'jitter') +\n",
    "  scale_color_brewer(type = 'div',\n",
    "                     guide = guide_legend(title = \"Color\", \n",
    "                                          reverse = F,\n",
    "                                          override.aes = list(alpha = 1, \n",
    "                                                              size = 2))) +  \n",
    "  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),\n",
    "                     breaks = c(0.2, 0.5, 1, 2, 3)) + \n",
    "  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),\n",
    "                     breaks = c(350, 1000, 5000, 10000, 15000)) +\n",
    "  ggtitle('Price (log10) by Cube-Root of Carat and Color')\n",
    "```\n",
    "“Did you remove reverse = T on the guide_legend? It’s a small detail that reminds us that D is the best color and J is worse for rating diamond colors.”\n",
    "Color and Price\n",
    "***\n",
    "\n",
    "### Color and Price\n",
    "Based on the the plot, do you think that the color of a diamond influences price? Why?\n",
    "Response: Keeping carat size constant, we see very clear division on the various diamond “color” categories similar to what we saw for “clarity”. Therefore, diamond color does have a clear influence on price.\n",
    "\n",
    "Solomon’s response: “Color does seem to explain some of the variance in price, just like we saw with the clarity variable. Blue Nile, a diamond retailer, however states that the difference between all color grades from D to J are basically not noticeable to the naked eye. Yet, we do see the color difference in the price tag.”\n",
    "\n",
    "Read more on Blue Nile’s thoughts about diamond color at: http://www.bluenile.com/education/diamonds/color.\n",
    "\n",
    "***\n",
    "\n",
    "### Linear Models in R\n",
    "Notes: To create a linear model in R, we will use the lm function. Price is the outcome and carat is the predictor variable. We use our domain knowledge of diamonds and carat weight to take the cube root of carat weight (volume). Therefore we use the following function in R: lm(log(price) ~ carat^(1/3)).\n",
    "\n",
    "Remember, we applied the log transformation to our long-tailed dollar variable and we speculated that the flawless diamond should become exponentially rarer as diamond volume increases. So we should be interested in the cube root of carat weight.\n",
    "\n",
    "Response:\n",
    "http://data.princeton.edu/R/linearModels.html\n",
    "***\n",
    "setRepositories(ind = c(1: 8))\n",
    "install.packages('mtable')\n",
    "library(mtable)\n",
    "# Error in mtable(m1, m2, m3, m4, m5, sdigits = 4) : \n",
    "  could not find function \"mtable\"\n",
    "  \n",
    "  output:\n",
    "## \n",
    "## Calls:\n",
    "## m1: lm(formula = I(log(price)) ~ I(carat^(1/3)), data = diamonds)\n",
    "## m2: lm(formula = I(log(price)) ~ I(carat^(1/3)) + carat, data = diamonds)\n",
    "## m3: lm(formula = I(log(price)) ~ I(carat^(1/3)) + carat + cut, data = diamonds)\n",
    "## m4: lm(formula = I(log(price)) ~ I(carat^(1/3)) + carat + cut + color, \n",
    "##     data = diamonds)\n",
    "## m5: lm(formula = I(log(price)) ~ I(carat^(1/3)) + carat + cut + color + \n",
    "##     clarity, data = diamonds)\n",
    "## \n",
    "## =========================================================================\n",
    "##                      m1         m2         m3         m4         m5      \n",
    "## -------------------------------------------------------------------------\n",
    "##   (Intercept)      2.821***   1.039***   0.874***   0.932***   0.415***  \n",
    "##                   (0.006)    (0.019)    (0.019)    (0.017)    (0.010)    \n",
    "##   I(carat^(1/3))   5.558***   8.568***   8.703***   8.438***   9.144***  \n",
    "##                   (0.007)    (0.032)    (0.031)    (0.028)    (0.016)    \n",
    "##   carat                      -1.137***  -1.163***  -0.992***  -1.093***  \n",
    "##                              (0.012)    (0.011)    (0.010)    (0.006)    \n",
    "##   cut: .L                                0.224***   0.224***   0.120***  \n",
    "##                                         (0.004)    (0.004)    (0.002)    \n",
    "##   cut: .Q                               -0.062***  -0.062***  -0.031***  \n",
    "##                                         (0.004)    (0.003)    (0.002)    \n",
    "##   cut: .C                                0.051***   0.052***   0.014***  \n",
    "##                                         (0.003)    (0.003)    (0.002)    \n",
    "##   cut: ^4                                0.018***   0.018***  -0.002     \n",
    "##                                         (0.003)    (0.002)    (0.001)    \n",
    "##   color: .L                                        -0.373***  -0.441***  \n",
    "##                                                    (0.003)    (0.002)    \n",
    "##   color: .Q                                        -0.129***  -0.093***  \n",
    "##                                                    (0.003)    (0.002)    \n",
    "##   color: .C                                         0.001     -0.013***  \n",
    "##                                                    (0.003)    (0.002)    \n",
    "##   color: ^4                                         0.029***   0.012***  \n",
    "##                                                    (0.003)    (0.002)    \n",
    "##   color: ^5                                        -0.016***  -0.003*    \n",
    "##                                                    (0.003)    (0.001)    \n",
    "##   color: ^6                                        -0.023***   0.001     \n",
    "##                                                    (0.002)    (0.001)    \n",
    "##   clarity: .L                                                  0.907***  \n",
    "##                                                               (0.003)    \n",
    "##   clarity: .Q                                                 -0.240***  \n",
    "##                                                               (0.003)    \n",
    "##   clarity: .C                                                  0.131***  \n",
    "##                                                               (0.003)    \n",
    "##   clarity: ^4                                                 -0.063***  \n",
    "##                                                               (0.002)    \n",
    "##   clarity: ^5                                                  0.026***  \n",
    "##                                                               (0.002)    \n",
    "##   clarity: ^6                                                 -0.002     \n",
    "##                                                               (0.002)    \n",
    "##   clarity: ^7                                                  0.032***  \n",
    "##                                                               (0.001)    \n",
    "## -------------------------------------------------------------------------\n",
    "##   R-squared            0.9        0.9        0.9        1.0        1.0   \n",
    "##   adj. R-squared       0.9        0.9        0.9        1.0        1.0   \n",
    "##   sigma                0.3        0.3        0.3        0.2        0.1   \n",
    "##   F               652012.1   387489.4   138654.5    87959.5   173791.1   \n",
    "##   p                    0.0        0.0        0.0        0.0        0.0   \n",
    "##   Log-likelihood   -7962.5    -3631.3    -1837.4     4235.2    34091.3   \n",
    "##   Deviance          4242.8     3613.4     3380.8     2699.2      892.2   \n",
    "##   AIC              15931.0     7270.6     3690.8    -8442.5   -68140.5   \n",
    "##   BIC              15957.7     7306.2     3762.0    -8317.9   -67953.7   \n",
    "##   N                53940      53940      53940      53940      53940    \n",
    "\n",
    "clarification wanted!!!!!\n",
    "Notice how adding cut to our model does not help explain much of the variance in the price of diamonds. This fits with out exploration earlier.\n",
    "\n",
    "### Building the Linear Model\n",
    "Notes: We build the linear model for price. Notice the I() wrapper around each variable of the first model. The I stands for 'as is'. In this case, it tells R to use the expression inside the I function to transform a variable before using it in the regression. This is instead of instructing R to interpret these symbols as part of the formula to construct the design matrix for the regression.\n",
    "\n",
    "Information on the lm model can be found at: http://data.princeton.edu/R/linearModels.html.\n",
    "\n",
    "We can also update the previous model to add the caret variable in the regression. The real functional relationship is surely not as simple as the cubed root of caret, so we add a simple linear function of caret in our model predicting price. We can continue to make more complex models by adding more variables. We add cut even though we don’t expect it to have much influence on price.\n",
    "\n",
    "Next, we add color to a 4th model and clarity to a 5th model.\n",
    "\n",
    "```{r Building the Linear Model}\n",
    "m1 <- lm(I(log(price)) ~ I(carat^(1/3)), data = diamonds)\n",
    "\n",
    "m2 <- update(m1, ~ . + carat)\n",
    "m3 <- update(m2, ~ . + cut)\n",
    "m4 <- update(m3, ~ . + color)\n",
    "m5 <- update(m4, ~ . + clarity)\n",
    "mtable(m1, m2, m3, m4, m5, sdigits=4)\n",
    "```\n",
    "setRepositories()\n",
    "\n",
    "\n",
    "Notice how adding cut to our model does not help explain much of the variance\n",
    "in the price of diamonds. This fits with out exploration earlier.\n",
    "\n",
    "***\n",
    "\n",
    "### Model Problems\n",
    "Video Notes:\n",
    " What could be some probelsm when using this model? What else should we think about when using this model? (Take some time to come up with 2-4 problems for the model) (You should 10-20 min on this)\n",
    "\n",
    "Response: This model only includes information about the prices for diamonds at a specific point in time (this data is from 2008). Therefore, the diamond market is probably quite different than it is now in 2016 (~8 year gap). Inflation and other things might have had a big effect on our model. Also, this predictive model might not be the most beneficial for the future based on the following information: “From 2018 onward, as existing mines get depleted and no major new deposits come online, supply is expected to decline, falling behind expected demand growth that will be driven by China, India and the US. Over the next 10-year period, supply and demand are expected to grow at a compound annual rate of 2.0% and 5.1%, respectively.” In other words, demand will surpass supply in the near future and the price of diamonds will rise exponentially! Therefore, we have to constantly update our model with the most recent set of diamond information and not use data that is 8 years old.\n",
    "\n",
    "The above quoted information was obtained from: http://www.bain.com/publications/articles/global-diamond-report-2013.aspx.\n",
    "\n",
    "Solomon’s Response: “To start, this data is from 2008. When I fitted models using this data and predicted the price of the diamonds that I found in the market, I kept getting predictions that were way too low. After some additional digging, I found that global diamonds were poor. It turns out that prices plummeted in 2008 due to the global financial crisis and since then, prices at least for wholesale polished diamonds, have grown at about 6% per year, compound annual rate… And finally, after looking at the data on price scope, I realized that diamond prices grew unevenly across different karat sizes since 2008. Therefore, the initially estimated model could not simply be adjusted by inflation.”\n",
    "\n",
    "Resources:\n",
    "\n",
    "    “Interpreting regression coefficient in R” on R Bloggers http://www.r-bloggers.com/interpreting-regression-coefficient-in-r/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29\n",
    "\n",
    "    “Interpreting Regression Coefficients” on The Analysis Factor http://www.theanalysisfactor.com/interpreting-regression-coefficients/\n",
    "\n",
    "    “Fitting and Interpreting Linear Models in R” on y.hat blog http://blog.yhat.com/posts/r-lm-summary.html\n",
    "\n",
    "\n",
    "Research:\n",
    "(Take some time to come up with 2-4 problems for the model)\n",
    "(You should 10-20 min on this)\n",
    "\n",
    "Response:\n",
    "\n",
    "***\n",
    "??t()\n",
    "diamondsurl = getBinaryURL(\"https://raw.github.com/solomonm/diamonds-data/master/BigDiamonds.Rda\")\n",
    "\n",
    "### A Bigger, Better Data Set\n",
    "Notes:\n",
    "\n",
    "```{r A Bigger, Better Data Set}\n",
    "#install.packages('bitops')\n",
    "#install.packages('RCurl')\n",
    "#library('bitops')\n",
    "#library('RCurl')\n",
    "\n",
    "#diamondsurl = getBinaryURL(\"https://github.com/SolomonMg/diamonds-data/blob/master/B#igDiamonds.Rda\", curl = #getCurlHandle())\n",
    "\n",
    "load(file = \"~/career_da-pffs/udacity-eda/eda-course-materials/lesson6/diamonds-data-master/BigDiamonds.Rda\")\n",
    "```\n",
    "\n",
    "The code used to obtain the data is available here:\n",
    "https://github.com/solomonm/diamonds-data\n",
    "\n",
    "## Building a Model Using the Big Diamonds Data Set\n",
    "Notes:# Your task is to build five linear models like Solomon\n",
    "# did for the diamonds data set only this\n",
    "# time you'll use a sample of diamonds from the\n",
    "# diamondsbig data set.\n",
    "\n",
    "# Be sure to make use of the same variables\n",
    "# (logprice, carat, etc.) and model\n",
    "# names (m1, m2, m3, m4, m5).\n",
    "\n",
    "# To get the diamondsbig data into RStudio\n",
    "# on your machine, copy, paste, and run the\n",
    "# code in the Instructor Notes. There's\n",
    "# 598,024 diamonds in this data set!\n",
    "\n",
    "# Since the data set is so large,\n",
    "# you are going to use a sample of the\n",
    "# data set to compute the models. You can use\n",
    "# the entire data set on your machine which\n",
    "# will produce slightly different coefficients\n",
    "# and statistics for the models.\n",
    "\n",
    "# This exercise WILL BE automatically graded.\n",
    "\n",
    "\n",
    "```{r Building a Model Using the Big Diamonds Data Set}\n",
    "#set.seed(20022012)\n",
    "\n",
    "#diamondsBigSample <- diamondsbig[sample(1:length(diamondsbig$price), #10000), ]\n",
    "\n",
    "#head(diamondsBigSample)\n",
    "\n",
    "# diamondsBigSample$logprice = log(diamondsBigSample$price)\n",
    "\n",
    "m1 <- lm(I(log(price)) ~ I(carat^(1/3)), data = diamondsBigSample)\n",
    "m2 <- update(m1, ~ . + carat)\n",
    "m3 <- update(m2, ~ . + cut)\n",
    "m4 <- update(m3, ~ . + color)\n",
    "m5 <- update(m4, ~ . + clarity)\n",
    "mtable(m1, m2, m3, m4, m5, sdigits=4)\n",
    "\n",
    "```\n",
    "great work!!\n",
    "\n",
    "***\n",
    "\n",
    "## Predictions\n",
    "Let’s use our newly created model to make predictions. Remember that we need to exponentiate our result because we are obtaining the prediction of price in (natural) logarithmic form.\n",
    "\n",
    "Example Diamond from BlueNile:\n",
    "Round 1.00 Very Good I VS1 $5,601\n",
    "\n",
    "```{r}\n",
    "#Be sure youâ€™ve loaded the library memisc and have m5 saved as an object in your workspace.\n",
    "thisDiamond = data.frame(carat = 1.00, cut = \"V.Good\",\n",
    "                         color = \"I\", clarity=\"VS1\")\n",
    "modelEstimate = predict(m5, newdata = thisDiamond,\n",
    "                        interval=\"prediction\", level = .95)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "modelEstimate\n",
    "\n",
    "exp(modelEstimate)\n",
    "```\n",
    "How could we do better? If we care most about diamonds with carat weights between 0.50 and 1.50, we might restrict the data we use to fit our model to diamonds that are that size - we have enough data.\n",
    "\n",
    "\n",
    "Evaluate how well the model predicts the BlueNile diamond's price. Think about the fitted point estimate as well as the 95% CI.\n",
    "\n",
    "Response:Our results show that the BlueNile diamond is a bit pricier ($5,601) than the expected value under the full model (Predicted Price = $5,071). However, it is by no means outside of the 95% CI. According to Solomon, “BlueNile has a better reputation than diamonSE.info. While this model might give you a sense of whether your diamond is a ripoff, against diamondSE.info diamonds, it’s not clear that diamondSE.info should be regarded as the universal source of truth about whether the price of the diamond is reasonable. Nonetheless, to have the expected price and an SE with a 95% confidence interval is a lot more information that we had about the price we should be willing to pay for a diamond before the start of this exercise.”\n",
    "\n",
    "#Side Note: The prediction interval here may be slightly conservative, as the model errors are heteroskedastic over carat (and hence price) even after our log and cube-root transformations.\n",
    "***\n",
    "See the output of the following code.\n",
    "```{r}\n",
    "dat = data.frame(m4$model, m4$residuals)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "with(dat, sd(m4.residuals))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#\n",
    "```{r}\n",
    "with(subset(dat, carat > .9 & carat < 1.1), sd(m4.residuals))\n",
    "\n",
    "dat$resid <- as.numeric(dat$m4.residuals)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "dat$resid <- as.numeric(dat$m4.residuals)\n",
    "\n",
    "ggplot(aes(y = resid, x = round(carat, 2)), data = dat) +\n",
    "  geom_line(stat = \"summary\", fun.y = sd) +\n",
    "  ggtitle(\"Model #5: Residuals vs. Carat Size\")\n",
    "```\n",
    "How could we do better? If we care most about diamonds with carat weights between 0.50 and 1.50, we might restrict the data we use to fit our model to diamonds that are that size - we have enough data. Notice how we get larger residuals for carat sizes greater than 2.0, which has an affect on the size of our confidence interval.\n",
    "\n",
    "\n",
    "\n",
    "## Final Thoughts\n",
    "Notes: Facebook data scientist Saung Masin brings up an important point: “even though we can predict the price of a diamond based on a function of the 4 c’s in diamonds, one thing that we shouldn’t conclude with this exercise is that where you buy your diamond is irrelevant. You will almost surely pay more for the same diamond at Tiffany’s compared to Costco. However, you can use a model like this to get a sense of whether you are overpaying. One last note is that data and models are never infallible and you can still get taken even equipped with this kind of analysis. There is no substitute for establishing a personal connection and a lasting business relationship with a jeweler you can trust.”\n",
    "\n",
    "***\n",
    "\n",
    "Click **KnitHTML** to see all of your hard work and to have an html\n",
    "page of this lesson, your answers, and your notes!\n",
    "\n",
    "https://rpubs.com/profversaggi/lesson_6_diamonds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
